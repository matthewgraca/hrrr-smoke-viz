{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c92945-842a-4338-9ec2-a954bca054a8",
   "metadata": {},
   "source": [
    "# Data Parameters\n",
    "(Basically, this will be deleted, as we keep iterating on this until we can get the next checkpoint)\n",
    "\n",
    "Input\n",
    "\n",
    "- Frames: 5 in 1 out\n",
    "- Target: 1 station (LA N. Main St.)\n",
    "- Channels: All\n",
    "\n",
    "Model\n",
    "- Variation\n",
    "```\n",
    "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
    "┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
    "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
    "│ conv_lstm2d (ConvLSTM2D)        │ (None, 5, 40, 40, 15)  │        11,400 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ conv_lstm2d_1 (ConvLSTM2D)      │ (None, 5, 40, 40, 30)  │        48,720 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ conv3d (Conv3D)                 │ (None, 5, 40, 40, 15)  │        12,165 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ conv3d_1 (Conv3D)               │ (None, 5, 40, 40, 1)   │           406 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ flatten (Flatten)               │ (None, 8000)           │             0 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ dense (Dense)                   │ (None, 1)              │         8,001 │\n",
    "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
    "│ reshape (Reshape)               │ (None, 1, 1)           │             0 │\n",
    "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
    "```\n",
    "\n",
    "Result\n",
    "- 52.8%\n",
    "\n",
    "General Notes\n",
    "- valid-test-train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a247a5-a0e5-46f7-adbd-3234c2f952d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-118.75, -117.0, 33.5, 34.5)\n",
      "Grid dimension: 40x40\n",
      "Frames per sample: 5\n"
     ]
    }
   ],
   "source": [
    "# file locations\n",
    "import sys\n",
    "sys.path.append(\"/home/mgraca/Workspace/hrrr-smoke-viz\") # for libs.plotting\n",
    "model_checkpoint_file = '/home/mgraca/Workspace/hrrr-smoke-viz/pwwb-experiments/tensorflow/experiment_output/basic_experiments/model.keras'\n",
    "data_path = \"/home/mgraca/Workspace/hrrr-smoke-viz/pwwb-experiments/tensorflow/final_input_data/two_yrs_daily/\"\n",
    "\n",
    "# Data parameters\n",
    "START_DATE = \"2022-12-01-00\"\n",
    "END_DATE = \"2024-12-01-00\"\n",
    "\n",
    "# Define bounding box\n",
    "lat_bottom, lat_top = 33.5, 34.5\n",
    "lon_bottom, lon_top = -118.75, -117.0\n",
    "extent = (lon_bottom, lon_top, lat_bottom, lat_top)\n",
    "\n",
    "# Input data shape\n",
    "dim = 40\n",
    "frames_per_sample = 5\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(f\"{extent}\")\n",
    "print(f\"Grid dimension: {dim}x{dim}\")\n",
    "print(f\"Frames per sample: {frames_per_sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_ingestion_title",
   "metadata": {},
   "source": [
    "# Data Ingestion and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "setup_dataset_manager",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "#from libs.sequence import PWWBPyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755a0fd0-5239-408b-b57c-83513e980468",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X_test_files =  [\"METAR_Wind_U_X_test.npy\", \"METAR_Wind_V_X_test.npy\", \"AirNow_PM25_X_test.npy\"]\n",
    "X_train_files = [\"METAR_Wind_U_X_train.npy\", \"METAR_Wind_V_X_train.npy\", \"AirNow_PM25_X_train.npy\"]\n",
    "X_valid_files = [\"METAR_Wind_U_X_valid.npy\", \"METAR_Wind_V_X_valid.npy\", \"AirNow_PM25_X_valid.npy\"]\n",
    "'''\n",
    "X_test_files =  [\"AirNow_PM25_X_test.npy\", \"HRRR_COLMD_X_test.npy\", \"MAIAC_AOD_X_test.npy\", \"METAR_Wind_U_X_test.npy\", \"METAR_Wind_V_X_test.npy\", \"TROPOMI_NO2_X_test.npy\"]\n",
    "X_train_files = [\"AirNow_PM25_X_train.npy\", \"HRRR_COLMD_X_train.npy\", \"MAIAC_AOD_X_train.npy\", \"METAR_Wind_U_X_train.npy\", \"METAR_Wind_V_X_train.npy\", \"TROPOMI_NO2_X_train.npy\"]\n",
    "X_valid_files = [\"AirNow_PM25_X_valid.npy\", \"HRRR_COLMD_X_valid.npy\", \"MAIAC_AOD_X_valid.npy\", \"METAR_Wind_U_X_valid.npy\", \"METAR_Wind_V_X_valid.npy\", \"TROPOMI_NO2_X_valid.npy\"]\n",
    "\n",
    "Y_test_files = \"Y_test.npy\"\n",
    "Y_train_files = \"Y_train.npy\"\n",
    "Y_valid_files = \"Y_valid.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "449e4a4f-7e3e-4725-99be-5bd86d7d3d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((727, 5, 40, 40, 6), (727, 5, 16))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' y is not trimmed, can't use this method\n",
    "BATCH_SIZE = 32\n",
    "X_training_files_paths = [f\"{path + file}\" for file in X_train_files]\n",
    "Y_training_files_paths = f\"{path + Y_train_files}\"\n",
    "generator = PWWBPyDataset(X_training_files_paths, Y_training_files_paths, BATCH_SIZE, workers=8, use_multiprocessing=True)\n",
    "\n",
    "history = seq.fit(\n",
    "    generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_valid_scaled, Y_valid),\n",
    "    callbacks=callbacks\n",
    ")\n",
    "'''\n",
    "X_train = np.stack([np.load(f\"{data_path + file}\") for file in X_train_files], axis=-1)\n",
    "X_test = np.stack([np.load(f\"{data_path + file}\") for file in X_test_files], axis=-1)\n",
    "X_valid = np.stack([np.load(f\"{data_path + file}\") for file in X_valid_files], axis=-1)\n",
    "\n",
    "Y_train = np.load(f\"{data_path + Y_train_files}\")\n",
    "Y_test = np.load(f\"{data_path + Y_test_files}\")\n",
    "Y_valid = np.load(f\"{data_path + Y_valid_files}\")\n",
    "\n",
    "X = np.concatenate([X_train, X_valid, X_test])\n",
    "Y = np.concatenate([Y_train, Y_valid, Y_test])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bde3a2-489c-4586-a3b5-e785b37c7be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 frames in, one frame and one station out\n",
    "# next frame = 0 (:1)\n",
    "# LA station = 4\n",
    "sensors = [\n",
    "    'Simi Valley - Cochran Street', 'Reseda', 'Santa Clarita', 'North Holywood', 'Los Angeles - N. Main Street', \n",
    "    'Compton', 'Long Beach Signal Hill', 'Anaheim', 'Glendora - Laurel', 'Mira Loma - Van Buren', \n",
    "    'Riverside - Rubidoux', 'Lake Elsinore - W. Flint Street', 'Crestline - Lake Gregory', \n",
    "    'Temecula (Lake Skinner)', 'Fontana - Arrow Highway', 'EBAM-2'\n",
    "]\n",
    "sensor = {location : i for i, location in enumerate(sensors)}\n",
    "\n",
    "Y_train = Y_train[:, :1, [sensor['North Holywood']]].copy()\n",
    "Y_test = Y_test[:, :1, [sensor['North Holywood']]].copy()\n",
    "Y_valid = Y_valid[:, :1, [sensor['North Holywood']]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3f3179-0564-45a0-afca-24da4610e0d4",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2c51e0-9d32-45eb-8abb-a3140089bf75",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa07e973-987e-4dd0-b12e-c7d32c96b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_frames, output_stations = Y_train.shape[1:]\n",
    "input_shape = X_train.shape[1:] # (frames, height, width, channels)\n",
    "\n",
    "print(\n",
    "    f\"X_train shape: {X_train.shape}\\n\"\n",
    "    f\"Y_train shape: {Y_train.shape}\\n\\n\"\n",
    "    \n",
    "    f\"X_test shape:  {X_test.shape}\\n\"\n",
    "    f\"Y_test shape:  {Y_test.shape}\\n\\n\"\n",
    "\n",
    "    f\"X_valid shape: {X_valid.shape}\\n\"\n",
    "    f\"Y_valid shape: {Y_valid.shape}\\n\\n\"\n",
    "\n",
    "    f\"Input shape:   {input_shape}\\n\"\n",
    "    f\"Output shape:  ({output_frames}, {output_stations})\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2534f19b-e4be-4e84-b374-7ae016ba14f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv3D\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.layers import BatchNormalization, LayerNormalization\n",
    "from keras.layers import Convolution2D, MaxPooling3D, Flatten, Reshape, GlobalAveragePooling2D\n",
    "from keras.layers import TimeDistributed, Reshape\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import InputLayer\n",
    "\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "seq = Sequential()\n",
    "\n",
    "seq.add(\n",
    "    InputLayer(shape=input_shape)\n",
    ")\n",
    "\n",
    "seq.add(\n",
    "    ConvLSTM2D(\n",
    "        filters=15, \n",
    "        kernel_size=(3, 3),\n",
    "        padding='same', \n",
    "        return_sequences=True,\n",
    "        #dropout=0.1,\n",
    "        #recurrent_dropout=0.1,\n",
    "        #kernel_regularizer=keras.regularizers.l2(0.01)\n",
    "    )\n",
    ")\n",
    "\n",
    "#seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(\n",
    "    ConvLSTM2D(\n",
    "        filters=30, \n",
    "        kernel_size=(3, 3),\n",
    "        padding='same', \n",
    "        return_sequences=True,\n",
    "        #dropout=0.1,\n",
    "        #recurrent_dropout=0.1,\n",
    "        #kernel_regularizer=keras.regularizers.l2(0.01)\n",
    "    )\n",
    ")\n",
    "\n",
    "#seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(\n",
    "    Conv3D(\n",
    "        filters=15, \n",
    "        kernel_size=(3, 3, 3),\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        #kernel_regularizer=keras.regularizers.l2(0.1)\n",
    "    )\n",
    ")\n",
    "\n",
    "#seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(\n",
    "    Conv3D(\n",
    "        filters=1, \n",
    "        kernel_size=(3, 3, 3),\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        #kernel_regularizer=keras.regularizers.l2(0.1)\n",
    "    )\n",
    ")\n",
    "\n",
    "#seq.add(TimeDistributed(Flatten()))\n",
    "seq.add(Flatten())\n",
    "\n",
    "#seq.add(Dropout(rate=0.1))\n",
    "\n",
    "seq.add(Dense(output_stations * output_frames, activation='relu'))\n",
    "\n",
    "seq.add(Reshape((output_frames, output_stations)))\n",
    "\n",
    "seq.compile(\n",
    "    loss='mean_absolute_error', \n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=0.001, weight_decay=0.004)\n",
    ")\n",
    "seq.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5809c1-c837-416c-ac38-5cf960332c51",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac857ae-26bb-4c01-b91c-2844e9b08dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint, ReduceLROnPlateau\n",
    "import os\n",
    "import time\n",
    "\n",
    "# tensorboard gradient logging callback\n",
    "# at the end of an epoch, pass in a sample batch and calculate the gradients\n",
    "class GradientLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, log_dir, data_sample):\n",
    "        super().__init__()\n",
    "        self.writer = tf.summary.create_file_writer(log_dir)\n",
    "        self.data_sample = data_sample  # A batch of (x, y) from your training data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # log gradients\n",
    "        x, y = self.data_sample\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.model.compute_loss(\n",
    "                y=y, \n",
    "                y_pred=self.model(x, training=True)\n",
    "            )\n",
    "\n",
    "        gradients = tape.gradient(loss, self.model.trainable_weights)\n",
    "        grad_norm = tf.linalg.global_norm(gradients)\n",
    "\n",
    "        # plot gradients\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.scalar(\"gradient_norm/global\", grad_norm, step=epoch)\n",
    "            \n",
    "            for weight, grad in zip(self.model.trainable_weights, gradients):\n",
    "                if grad is not None:\n",
    "                    tf.summary.histogram(f\"gradients/{weight.name}\", grad, step=epoch)\n",
    "            self.writer.flush()\n",
    "\n",
    "# tensorboard callback setup\n",
    "def get_run_logdir(\n",
    "    root_logdir, \n",
    "    run_id=time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "):\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "    \n",
    "run_logdir = get_run_logdir(\n",
    "    root_logdir=os.path.join(os.curdir, \"my_logs\"),\n",
    "    run_id=\"default\"\n",
    ")\n",
    "tensorboard_callback = TensorBoard(run_logdir, histogram_freq=1)\n",
    "#gradient_logging_callback = GradientLogger(run_logdir, data_sample=(X_test_scaled[:32], Y_test[:32]))\n",
    "\n",
    "# model checkpoint callback setup\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=model_checkpoint_file,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "# lr scheduling callback setup\n",
    "lr_scheduling_callback = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.1,\n",
    "    patience=10,\n",
    ")\n",
    "\n",
    "# early stopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "# define list of callbacks we're gonna use\n",
    "callbacks = [\n",
    "    early_stopping_callback,\n",
    "    model_checkpoint_callback,\n",
    "    #lr_scheduling_callback,\n",
    "    #tensorboard_callback,\n",
    "    #gradient_logging_callback\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67953bd6-fb5d-435d-9597-9fc2a03f6d4f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a354fa-43ec-4dd8-9d8f-abbf4ba70c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = seq.fit(\n",
    "    x=X_train,\n",
    "    y=Y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_valid, Y_valid),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06040dc-c1d1-43f9-84d0-60b5cd4ea11c",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f818b07b-5447-40ed-b5bc-626d29d8e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from datetime import datetime\n",
    "\n",
    "# Evaluate model\n",
    "print(f\"\\nEvaluating model\")\n",
    "#model = keras.saving.load_model(model_checkpoint_file)\n",
    "y_pred = seq.predict(X_test, verbose=0)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title(f'\\nTraining Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MAE)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13abe848-4d29-46ce-af43-58823627b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.plotting import *\n",
    "import pandas as pd\n",
    "\n",
    "results = [\n",
    "    [\n",
    "        rmse(Y_test[1:], Y_test[:-1]), \n",
    "        rmse(y_pred, Y_test),\n",
    "        rmse(y_pred[1:], Y_test[:-1]),\n",
    "    ],\n",
    "    [\n",
    "        nrmse(Y_test[1:], Y_test[:-1]),\n",
    "        nrmse(y_pred, Y_test),        \n",
    "        nrmse(y_pred[1:], Y_test[:-1]),\n",
    "\n",
    "    ],\n",
    "    [        \n",
    "        mae(Y_test[1:], Y_test[:-1]),\n",
    "        mae(y_pred, Y_test),\n",
    "        mae(y_pred[1:], Y_test[:-1]),\n",
    "    ],\n",
    "    [\n",
    "        r2_score(Y_test[:-1], Y_test[1:]),\n",
    "        r2_score(Y_test, y_pred),\n",
    "        r2_score(Y_test[:-1], y_pred[1:]),\n",
    "    ]\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    results, \n",
    "    columns=['Baseline Error', 'Model Error', 'Model Error Shifted'], \n",
    "    index=['RMSE', 'NRMSE', 'MAE', 'R2']\n",
    ")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eec4d1-0b82-4b4a-a1f9-7ac59f4d27d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first-frame error\n",
    "results = [\n",
    "    [\n",
    "        rmse(Y_test[1:, 0], Y_test[:-1, 0]), \n",
    "        rmse(y_pred[:, 0], Y_test[:, 0]),\n",
    "        rmse(y_pred[1:, 0], Y_test[:-1, 0]),\n",
    "    ],\n",
    "    [\n",
    "        nrmse(Y_test[1:, 0], Y_test[:-1, 0]),\n",
    "        nrmse(y_pred[:, 0], Y_test[:, 0]),        \n",
    "        nrmse(y_pred[1:, 0], Y_test[:-1, 0]),\n",
    "\n",
    "    ],\n",
    "    [        \n",
    "        mae(Y_test[1:, 0], Y_test[:-1, 0]),\n",
    "        mae(y_pred[:, 0], Y_test[:, 0]),\n",
    "        mae(y_pred[1:, 0], Y_test[:-1, 0]),\n",
    "    ],\n",
    "    [\n",
    "        r2_score(Y_test[:-1, 0], Y_test[1:, 0]),\n",
    "        r2_score(Y_test[:, 0], y_pred[:, 0]),\n",
    "        r2_score(Y_test[:-1, 0], y_pred[1:, 0]),\n",
    "    ]\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    results, \n",
    "    columns=['Baseline Error', 'Model Error', 'Model Error Shifted'], \n",
    "    index=['RMSE', 'NRMSE', 'MAE', 'R2']\n",
    ")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59334766-90f5-4c9a-89e5-39803da5af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up dead sensors; more accurate, also allows us to use mape\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "#mape_err = mape(np.squeeze(cleaned_test), np.squeeze(cleaned_pred))\n",
    "\n",
    "dead_sens = np.where(Y_test == 0)\n",
    "print(f\"{len(dead_sens[0])} samples containing dead sensors removed\")\n",
    "\n",
    "cleaned_test = np.delete(Y_test, dead_sens, 0)\n",
    "cleaned_pred = np.delete(y_pred, dead_sens, 0)\n",
    "\n",
    "results = [\n",
    "    [\n",
    "        rmse(cleaned_test[1:], cleaned_test[:-1]), \n",
    "        rmse(cleaned_pred, cleaned_test),\n",
    "        rmse(cleaned_pred[1:], cleaned_test[:-1]),\n",
    "    ],\n",
    "    [\n",
    "        nrmse(cleaned_test[1:], cleaned_test[:-1]),\n",
    "        nrmse(cleaned_pred, cleaned_test),        \n",
    "        nrmse(cleaned_pred[1:], cleaned_test[:-1]),\n",
    "\n",
    "    ],\n",
    "    [        \n",
    "        mae(cleaned_test[1:], cleaned_test[:-1]),\n",
    "        mae(cleaned_pred, cleaned_test),\n",
    "        mae(cleaned_pred[1:], cleaned_test[:-1]),\n",
    "    ],\n",
    "    [\n",
    "        mape(np.squeeze(cleaned_test[:-1]), np.squeeze(cleaned_test[1:]))*100,\n",
    "        mape(np.squeeze(cleaned_test), np.squeeze(cleaned_pred))*100,\n",
    "        mape(np.squeeze(cleaned_test[:-1]), np.squeeze(cleaned_pred[1:]))*100,\n",
    "    ],\n",
    "    [\n",
    "        r2_score(cleaned_test[:-1], cleaned_test[1:]),\n",
    "        r2_score(cleaned_test, cleaned_pred),\n",
    "        r2_score(cleaned_test[:-1], cleaned_pred[1:]),\n",
    "    ]\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    results, \n",
    "    columns=['Baseline Error', 'Model Error', 'Model Error Shifted'], \n",
    "    index=['RMSE', 'NRMSE', 'MAE', 'MAPE', 'R2']\n",
    ")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb78200-9687-4674-9589-284647ec0f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first frame results\n",
    "results = [\n",
    "    [\n",
    "        rmse(cleaned_test[1:, 0], cleaned_test[:-1, 0]), \n",
    "        rmse(cleaned_pred[:, 0], cleaned_test[:, 0]),\n",
    "        rmse(cleaned_pred[1:, 0], cleaned_test[:-1, 0]),\n",
    "    ],\n",
    "    [\n",
    "        nrmse(cleaned_test[1:, 0], cleaned_test[:-1, 0]),\n",
    "        nrmse(cleaned_pred[:, 0], cleaned_test[:, 0]),        \n",
    "        nrmse(cleaned_pred[1:, 0], cleaned_test[:-1, 0]),\n",
    "\n",
    "    ],\n",
    "    [        \n",
    "        mae(cleaned_test[1:, 0], cleaned_test[:-1, 0]),\n",
    "        mae(cleaned_pred[:, 0], cleaned_test[:, 0]),\n",
    "        mae(cleaned_pred[1:, 0], cleaned_test[:-1, 0]),\n",
    "    ],\n",
    "    [\n",
    "        mape(np.squeeze(cleaned_test[:-1, 0]), np.squeeze(cleaned_test[1:, 0]))*100,\n",
    "        mape(np.squeeze(cleaned_test[:, 0]), np.squeeze(cleaned_pred[:, 0]))*100,\n",
    "        mape(np.squeeze(cleaned_test[:-1, 0]), np.squeeze(cleaned_pred[1:, 0]))*100,\n",
    "    ],\n",
    "    [\n",
    "        r2_score(cleaned_test[:-1, 0], cleaned_test[1:, 0]),\n",
    "        r2_score(cleaned_test[:, 0], cleaned_pred[:, 0]),\n",
    "        r2_score(cleaned_test[:-1, 0], cleaned_pred[1:, 0]),\n",
    "    ]\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    results, \n",
    "    columns=['Baseline Error', 'Model Error', 'Model Error Shifted'], \n",
    "    index=['RMSE', 'NRMSE', 'MAE', 'MAPE', 'R2']\n",
    ")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72cabf4-452a-4a18-b156-29ff89e961a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_names = ['Los Angeles - N. Main Street']\n",
    "plot_frame_by_frame_rmse(y_pred, Y_test)\n",
    "print_detailed_frame_stats(y_pred, Y_test, sensor_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899b3f9e-8c84-4b7c-a2ca-ba52bf1238cc",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f873ee-8c78-4433-a302-695b4126acf5",
   "metadata": {},
   "source": [
    "## Snapshot plots - first frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4c271d-757b-4aa1-8009-5927fb78ff9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred[:7, 0], linestyle=':', label='pred')\n",
    "plt.plot(Y_test[:7, 0], label='actual')\n",
    "plt.plot(y_pred[1:8, 0], linestyle='--', label='pred offset')\n",
    "plt.xlabel(\"Time (hourly)\")\n",
    "plt.ylabel(\"PM2.5 Concentrations\")\n",
    "plt.title(\"1 week snapshot of predictions\")\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc90835-d5bd-4aba-9997-014ae6d66701",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred[:30, 0], label='pred')\n",
    "plt.plot(Y_test[:30, 0], label='actual')\n",
    "plt.xlabel(\"Time (hourly)\")\n",
    "plt.ylabel(\"PM2.5 Concentrations\")\n",
    "plt.title(\"30 day snapshot of predictions\")\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e897a81-e3f9-4e93-acbc-1ef451e897d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEEK = 7\n",
    "best_week = float('inf')\n",
    "worst_week = float('-inf')\n",
    "best_idx, worst_idx = 0, 0\n",
    "\n",
    "worst_week_cleaned = float('-inf')\n",
    "worst_idx_cleaned = 0\n",
    "for i in range(len(y_pred) - WEEK):\n",
    "    err = nrmse(y_pred[i:i+WEEK, 0], Y_test[i:i+WEEK, 0])\n",
    "    if err < best_week:\n",
    "        best_week = err\n",
    "        best_idx = i\n",
    "    if err > worst_week:\n",
    "        worst_week = err\n",
    "        worst_idx = i\n",
    "\n",
    "for i in range(len(cleaned_pred) - WEEK):\n",
    "    cleaned_err = nrmse(cleaned_pred[i:i+WEEK], cleaned_test[i:i+WEEK])\n",
    "    if cleaned_err > worst_week_cleaned:\n",
    "        worst_week_cleaned = cleaned_err\n",
    "        worst_idx_cleaned = i\n",
    "\n",
    "print(len(y_pred), len(cleaned_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8838bd35-a5f1-4e58-b21d-c70ecff3f471",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range(START_DATE, END_DATE, freq='D')[:-frames_per_sample]\n",
    "y_test_start_idx = len(X_train) + len(X_valid)\n",
    "print(len(Y_train), len(Y_valid), len(Y_test), len(y_pred), len(dates))\n",
    "print(len(Y_train) + len(Y_valid) + len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deebda0d-144f-464f-af90-f6ce86e54fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(18, 10))\n",
    "fig.suptitle('Best and worst week (first frame) of predictions')\n",
    "\n",
    "# best week\n",
    "axes[0].plot(\n",
    "    dates[y_test_start_idx + best_idx : y_test_start_idx + best_idx + WEEK],\n",
    "    y_pred[best_idx:best_idx+WEEK, 0],\n",
    "    label='pred'\n",
    ")\n",
    "axes[0].plot(\n",
    "    dates[y_test_start_idx + best_idx : y_test_start_idx + best_idx + WEEK],\n",
    "    Y_test[best_idx:best_idx+WEEK, 0],\n",
    "    label='actual'\n",
    ")\n",
    "axes[0].set_xlabel(\"Time (hourly)\")\n",
    "axes[0].set_ylabel(\"PM2.5 Concentrations\")\n",
    "axes[0].set_title(\n",
    "    f\"Best week: {dates[y_test_start_idx + best_idx].strftime('%Y-%m-%d')} to \"\n",
    "    f\"{dates[y_test_start_idx + best_idx + WEEK].strftime('%Y-%m-%d')}\\n\"\n",
    "    f\"NRMSE: {best_week:.2f}%, \"\n",
    "    f\"MAPE: {mape(\n",
    "        np.squeeze(Y_test[best_idx : best_idx + WEEK]), \n",
    "        np.squeeze(y_pred[best_idx : best_idx + WEEK])\n",
    "    )*100:.2f}%\"\n",
    ")\n",
    "axes[0].legend(loc=\"upper right\")\n",
    "\n",
    "# worst week (cleaned)\n",
    "axes[1].plot(\n",
    "    dates[y_test_start_idx + worst_idx_cleaned : y_test_start_idx + worst_idx_cleaned + WEEK],\n",
    "    cleaned_pred[worst_idx_cleaned:worst_idx_cleaned+WEEK, 0], \n",
    "    label='pred'\n",
    ")\n",
    "axes[1].plot(\n",
    "    dates[y_test_start_idx + worst_idx_cleaned : y_test_start_idx + worst_idx_cleaned + WEEK],\n",
    "    cleaned_test[worst_idx_cleaned:worst_idx_cleaned+WEEK, 0],\n",
    "    label='actual'\n",
    ")\n",
    "axes[1].set_xlabel(\"Time (hourly)\")\n",
    "axes[1].set_ylabel(\"PM2.5 Concentrations\")\n",
    "axes[1].set_title(\n",
    "    f\"Worst 1 week (cleaned): {dates[y_test_start_idx + worst_idx_cleaned].strftime('%Y-%m-%d')} to \"\n",
    "    f\"{dates[y_test_start_idx + worst_idx_cleaned + WEEK].strftime('%Y-%m-%d')}\\n\"\n",
    "    f\"NRMSE: {worst_week_cleaned:.2f}%, \"\n",
    "    f\"MAPE: {mape(\n",
    "        np.squeeze(Y_test[worst_idx_cleaned : worst_idx_cleaned + WEEK]), \n",
    "        np.squeeze(y_pred[worst_idx_cleaned : worst_idx_cleaned + WEEK])\n",
    "    )*100:.2f}%\"\n",
    ")\n",
    "axes[0].legend(loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea177c5a-aca2-4d48-bf94-ff5734320333",
   "metadata": {},
   "source": [
    "## Rolling avg plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0728f15f-598b-43a8-8542-6c8d96eb1d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_pred = pd.Series(np.squeeze(y_pred[:, 0]))\n",
    "series_test = pd.Series(np.squeeze(Y_test[:, 0]))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dates[y_test_start_idx:], series_pred, label=f'pred')\n",
    "plt.plot(dates[y_test_start_idx:], series_test, label=f'actual')\n",
    "plt.title('Time Series (exact)')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('PM2.5')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a2baaf-3078-4e55-8243-bc6f59c6aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "window_size = 7 # weekly average\n",
    "smoothed_pred = series_pred.rolling(window=window_size).mean()\n",
    "smoothed_test = series_test.rolling(window=window_size).mean()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(smoothed_pred, label=f'pred')\n",
    "plt.plot(smoothed_test, label=f'actual')\n",
    "plt.title('Time Series with 7-day Moving Average Smoothing')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('PM2.5 Weekly Average')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2580517-de99-468c-bb37-178fd6d7d1b0",
   "metadata": {},
   "source": [
    "## Error plots (first frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1798928-8873-443a-9bdd-ceb1ed4eb705",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = pd.Series(np.squeeze(np.abs(y_pred[:, 0] - Y_test[:, 0])))\n",
    "window_size = WEEK # weekly average\n",
    "smoothed_series = series.rolling(window=window_size).mean()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(series, label=f'daily error', color='red')\n",
    "plt.plot(smoothed_series, label=f'weekly error', color='blue')\n",
    "plt.title('Time Series of error')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('PM2.5')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c03af-fce7-43e1-916d-9aafb27dc826",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
