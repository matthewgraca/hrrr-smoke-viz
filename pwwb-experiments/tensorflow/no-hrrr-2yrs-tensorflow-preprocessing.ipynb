{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deca3910-8b85-4d35-ba11-4278b6cfa502",
   "metadata": {},
   "source": [
    "# Air Quality Prediction Experiment\n",
    "\n",
    "This notebook combines PWWB, AirNow, and HRRR datasets for air quality prediction using TF-Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiment_config",
   "metadata": {},
   "source": [
    "# Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "experiment_globals",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: January 2022 to December 2024\n",
      "Description: January 2025 fire period with PWWB and Airnow data.\n",
      "Date Range: 2022-12-01-00 to 2024-12-01-00\n",
      "Train/Test Split: 70%/30% (temporal)\n"
     ]
    }
   ],
   "source": [
    "# ========== EXPERIMENT CONFIGURATION ==========\n",
    "# Change these variables to configure your experiment\n",
    "\n",
    "EXPERIMENT_NAME = \"January 2022 to December 2024\"\n",
    "EXPERIMENT_DESCRIPTION = \"January 2025 fire period with PWWB and Airnow data.\"\n",
    "EXPERIMENT_ID = \"two_years_pwwb_airnow_hrrr\"\n",
    "\n",
    "# Data parameters\n",
    "START_DATE = \"2022-12-01-00\"\n",
    "END_DATE = \"2024-12-01-00\"\n",
    "TRAIN_SPLIT = 0.70  # 70% for training, 10% for val (will split later), leaving 20% for test.\n",
    "\n",
    "# Model parameters\n",
    "EPOCHS = 500\n",
    "BATCH_SIZE = 16\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "print(f\"Experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"Description: {EXPERIMENT_DESCRIPTION}\")\n",
    "print(f\"Date Range: {START_DATE} to {END_DATE}\")\n",
    "print(f\"Train/Test Split: {TRAIN_SPLIT*100:.0f}%/{(1-TRAIN_SPLIT)*100:.0f}% (temporal)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c92945-842a-4338-9ec2-a954bca054a8",
   "metadata": {},
   "source": [
    "# Data Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95a247a5-a0e5-46f7-adbd-3234c2f952d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-118.75, -117.0, 33.5, 34.5)\n",
      "Grid dimension: 40x40\n",
      "Frames per sample: 5\n"
     ]
    }
   ],
   "source": [
    "# Define bounding box\n",
    "lat_bottom, lat_top = 33.5, 34.5\n",
    "lon_bottom, lon_top = -118.75, -117.0\n",
    "extent = (lon_bottom, lon_top, lat_bottom, lat_top)\n",
    "\n",
    "# Input data shape\n",
    "dim = 40\n",
    "frames_per_sample = 5\n",
    "\n",
    "print(f\"{extent}\")\n",
    "print(f\"Grid dimension: {dim}x{dim}\")\n",
    "print(f\"Frames per sample: {frames_per_sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_ingestion_title",
   "metadata": {},
   "source": [
    "# Data Ingestion and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28cd894d-b165-4b11-9d41-0608bdf6783e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mgraca/miniconda3/envs/hrrrenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Python nonsense that allows you to import from sibling directories\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import the new PWWB implementation and dataset manager\n",
    "from libs.pwwb import PWWBData\n",
    "from libs.pwwb.utils.dataset_manager import create_dataset_manager\n",
    "\n",
    "# Import the AirNow data class\n",
    "from libs.airnowdata import AirNowData\n",
    "\n",
    "# Load environment variables (API keys, credentials)\n",
    "load_dotenv()\n",
    "\n",
    "import numpy as np\n",
    "# Temporal train-test split function\n",
    "def temporal_train_test_split(X, Y, train_size=0.8):\n",
    "    \"\"\"Split data temporally - first portion for training, last portion for testing\"\"\"\n",
    "    split_idx = int(X.shape[0] * train_size)\n",
    "    \n",
    "    X_train = X[:split_idx]\n",
    "    X_test = X[split_idx:]\n",
    "    Y_train = Y[:split_idx]\n",
    "    Y_test = Y[split_idx:]\n",
    "    \n",
    "    print(f\"Temporal split at index {split_idx}:\")\n",
    "    print(f\"  Training: samples 0-{split_idx-1} ({train_size*100:.0f}% of time)\")\n",
    "    print(f\"  Testing: samples {split_idx}-{len(X)-1} ({(1-train_size)*100:.0f}% of time)\")\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "    \n",
    "def temporal_train_test_valid_split(X, Y, train_size=0.7, test_size=0.2, valid_size=0.1):\n",
    "    train_split = int(X.shape[0] * train_size)\n",
    "    test_split = int(X.shape[0] * (train_size + valid_size))\n",
    "    \n",
    "    X_train = X[:train_split]\n",
    "    X_valid = X[train_split:test_split]\n",
    "    X_test = X[test_split:]\n",
    "    Y_train = Y[:train_split]\n",
    "    Y_valid = Y[train_split:test_split]\n",
    "    Y_test = Y[test_split:]\n",
    "    \n",
    "    print(f\"Temporal split at indices {train_split} and {test_split}:\")\n",
    "    print(f\"  Training: samples 0-{train_split-1} ({train_size*100:.0f}% of time)\")\n",
    "    print(f\"  Validation: samples {train_split}-{test_split-1} ({valid_size*100:.0f}% of time)\")\n",
    "    print(f\"  Testing: samples {test_split}-{len(X)-1} ({test_size*100:.0f}% of time)\")\n",
    "    \n",
    "    return X_train, X_test, X_valid, Y_train, Y_test, Y_valid\n",
    "\n",
    "# scale training data, then scale test data based on training data stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def std_scale(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_train = scaler.fit_transform(X_train.reshape(-1, 1)).reshape(X_train.shape)\n",
    "    scaled_test = scaler.transform(X_test.reshape(-1, 1)).reshape(X_test.shape)\n",
    "\n",
    "    return scaled_train, scaled_test\n",
    "\n",
    "def std_scale_train_test_valid(X_train, X_test, X_valid):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_train = scaler.fit_transform(X_train.reshape(-1, 1)).reshape(X_train.shape)\n",
    "    scaled_test = scaler.transform(X_test.reshape(-1, 1)).reshape(X_test.shape)\n",
    "    scaled_valid = scaler.transform(X_valid.reshape(-1, 1)).reshape(X_valid.shape)\n",
    "\n",
    "    return scaled_train, scaled_test, scaled_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "setup_dataset_manager",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing datasets:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>created</th>\n",
       "      <th>description</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>channels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>two_years_pwwb_airnow_hrrr_MAIAC_TROPOMI_NO2_M...</td>\n",
       "      <td>2025-05-31T14:47:49.908325</td>\n",
       "      <td>January 2025 fire period with PWWB, Airnow, an...</td>\n",
       "      <td>2022-12-01-00</td>\n",
       "      <td>2024-12-01-00</td>\n",
       "      <td>maiac, tropomi, metar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  two_years_pwwb_airnow_hrrr_MAIAC_TROPOMI_NO2_M...   \n",
       "\n",
       "                      created  \\\n",
       "0  2025-05-31T14:47:49.908325   \n",
       "\n",
       "                                         description     start_date  \\\n",
       "0  January 2025 fire period with PWWB, Airnow, an...  2022-12-01-00   \n",
       "\n",
       "        end_date               channels  \n",
       "0  2024-12-01-00  maiac, tropomi, metar  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create output directory for results\n",
    "output_dir = f\"experiment_output/{EXPERIMENT_ID}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create dataset manager\n",
    "manager = create_dataset_manager(\n",
    "    registry_file=f\"{EXPERIMENT_ID}_registry.json\",\n",
    "    cache_dir=\"data/pwwb_cache/\"\n",
    ")\n",
    "\n",
    "# List existing datasets\n",
    "print(\"Existing datasets:\")\n",
    "try:\n",
    "    display(manager.list_datasets())\n",
    "except:\n",
    "    print(\"No existing datasets found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "process_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading PWWB data for January 2022 to December 2024...\n",
      "Dataset 'two_years_pwwb_airnow_hrrr_MAIAC_TROPOMI_NO2_METAR_WIND_UV' already exists, loading from cache...\n",
      "Using cache prefix: two_years_pwwb_airnow_hrrr_MAIAC_TROPOMI_NO2_METAR_WIND_UV_\n",
      "Initialized PWWBData with 17544 hourly timestamps\n",
      "Date range: 2022-12-01 00:00:00 to 2024-11-30 23:00:00\n",
      "Channels included: ['maiac', 'tropomi', 'metar']\n",
      "TROPOMI channels: ['TROPOMI_NO2']\n",
      "METAR channels: ['METAR_Wind_U', 'METAR_Wind_V']\n",
      "Processing MAIAC AOD data...\n",
      "Loading cached data from data/pwwb_cache/two_years_pwwb_airnow_hrrr_MAIAC_TROPOMI_NO2_METAR_WIND_UV_maiac_aod_data.npy\n",
      "Processing TROPOMI data...\n",
      "Including TROPOMI channels: ['TROPOMI_NO2']\n",
      "Loading cached data from data/pwwb_cache/two_years_pwwb_airnow_hrrr_MAIAC_TROPOMI_NO2_METAR_WIND_UV_tropomi_no2_data.npy\n",
      "Processing METAR meteorological data...\n",
      "Elevation data not found at inputs/elevation.npy. Using flat elevation.\n",
      "Initialized MetarDataSource with 2 channels: ['METAR_Wind_U', 'METAR_Wind_V']\n",
      "Will fetch these raw variables: ['sped', 'drct']\n",
      "Will calculate wind U/V components from speed/direction\n",
      "Loading cached data from data/pwwb_cache/two_years_pwwb_airnow_hrrr_MAIAC_TROPOMI_NO2_METAR_WIND_UV_metar_u_v_data.npy\n",
      "Final data shape: (17540, 5, 40, 40, 4)\n",
      "\n",
      "Channel Statistics:\n",
      "===================\n",
      "\n",
      "Channel 0: MAIAC_AOD\n",
      "  Min: -1.0\n",
      "  Max: 0.5259743928909302\n",
      "  Mean: -0.5767326574510662\n",
      "  Std: 0.544402011749594\n",
      "  Data coverage: 100.00% (1600/1600 non-zero pixels)\n",
      "\n",
      "Channel 1: TROPOMI_NO2\n",
      "  Min: 2.5132922952494283e-05\n",
      "  Max: 0.00017178806736547404\n",
      "  Mean: 8.00315841671379e-05\n",
      "  Std: 2.5192895807387743e-05\n",
      "  Data coverage: 100.00% (1600/1600 non-zero pixels)\n",
      "\n",
      "Channel 2: METAR_Wind_U\n",
      "  Min: 3.4740779667173873\n",
      "  Max: 4.433922992082574\n",
      "  Mean: 3.9388995343038595\n",
      "  Std: 0.181296835244206\n",
      "  Data coverage: 100.00% (1600/1600 non-zero pixels)\n",
      "\n",
      "Channel 3: METAR_Wind_V\n",
      "  Min: 5.1762162201025035\n",
      "  Max: 5.5576847858683465\n",
      "  Mean: 5.373019625671163\n",
      "  Std: 0.04018928210291189\n",
      "  Data coverage: 100.00% (1600/1600 non-zero pixels)\n",
      "\n",
      "Final Data Shape:\n",
      "  17540 samples\n",
      "  5 frames per sample\n",
      "  40x40 grid size\n",
      "  4 channels\n",
      "\n",
      "Data Memory Usage:\n",
      "  4282.23 MB\n",
      "Loaded channel info from data/pwwb_cache/two_years_pwwb_airnow_hrrr_MAIAC_TROPOMI_NO2_METAR_WIND_UV_full_data_channel_info.json\n",
      "Channels: ['MAIAC_AOD', 'TROPOMI_NO2', 'METAR_Wind_U', 'METAR_Wind_V']\n",
      "Data loaded from data/pwwb_cache/two_years_pwwb_airnow_hrrr_MAIAC_TROPOMI_NO2_METAR_WIND_UV_full_data.npy\n",
      "Data shape: (17540, 5, 40, 40, 4)\n",
      "✓ PWWB data shape: (17540, 5, 40, 40, 4)\n",
      "  Channels: ['MAIAC_AOD', 'TROPOMI_NO2', 'METAR_Wind_U', 'METAR_Wind_V']\n",
      "\n",
      "Loading AirNow data for January 2022 to December 2024...\n",
      "Elevation data not found at ../libs/inputs/elevation.npy. Using flat elevation.\n",
      "Mask usage disabled. All sensors within extent will be included.\n",
      "Loading processed AirNow data from cache: data/airnow_processed.npz\n",
      "✓ Successfully loaded processed data from cache\n",
      "  - Data shape: (17540, 5, 40, 40, 1)\n",
      "  - Found 8 sensor locations\n",
      "✓ AirNow data shape: (17540, 5, 40, 40, 1)\n",
      "  Target stations shape: (17535, 5, 8)\n",
      "  Number of sensors: 8\n",
      "  Sensor names: ['Simi Valley - Cochran Street', 'Reseda', 'Santa Clarita', 'North Holywood', 'Los Angeles - N. Main Street', 'Compton', 'Long Beach Signal Hill', 'Glendora - Laurel']\n"
     ]
    }
   ],
   "source": [
    "# Adjust end date for AirNow\n",
    "end_date_adj = pd.to_datetime(END_DATE) - pd.Timedelta(hours=1)\n",
    "\n",
    "# Dataset name and description\n",
    "dataset_name = f\"{EXPERIMENT_ID}_MAIAC_TROPOMI_NO2_METAR_WIND_UV\"\n",
    "dataset_desc = f\"{EXPERIMENT_DESCRIPTION} - MAIAC, TROPOMI NO2, METAR Wind U/V components\"\n",
    "\n",
    "# ========== 1. Load PWWB Data ==========\n",
    "print(f\"\\nLoading PWWB data for {EXPERIMENT_NAME}...\")\n",
    "\n",
    "# Check if dataset already exists in the registry\n",
    "if manager.get_dataset_info(dataset_name) is not None:\n",
    "    print(f\"Dataset '{dataset_name}' already exists, loading from cache...\")\n",
    "    pwwb_data = manager.load_dataset(dataset_name, PWWBData)\n",
    "else:\n",
    "    print(f\"Dataset '{dataset_name}' not found, creating new one...\")\n",
    "    # Create the dataset with the specified channels\n",
    "    pwwb_data = manager.create_dataset(\n",
    "        name=dataset_name,\n",
    "        description=dataset_desc,\n",
    "        PWWBData_class=PWWBData,\n",
    "        start_date=START_DATE,\n",
    "        end_date=END_DATE,\n",
    "        extent=extent,\n",
    "        frames_per_sample=frames_per_sample,\n",
    "        dim=dim,\n",
    "        include_channels={\n",
    "            'maiac': True,                     # Include MAIAC AOD\n",
    "            'tropomi': ['TROPOMI_NO2'],        # Only include NO2 from TROPOMI\n",
    "            'metar': ['METAR_Wind_U', 'METAR_Wind_V'],  # Only wind components from METAR\n",
    "            'modis_fire': False,               # Exclude MODIS fire data\n",
    "            'merra2': False                    # Exclude MERRA2 data\n",
    "        },\n",
    "        verbose=True,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    # Save the dataset\n",
    "    pwwb_data.save_data()\n",
    "\n",
    "# Get the data and channel info\n",
    "X_pwwb = pwwb_data.data\n",
    "channel_info = pwwb_data.get_channel_info()\n",
    "print(f\"✓ PWWB data shape: {X_pwwb.shape}\")\n",
    "print(f\"  Channels: {channel_info['channel_names']}\")\n",
    "\n",
    "# ========== 2. Load AirNow Data ==========\n",
    "print(f\"\\nLoading AirNow data for {EXPERIMENT_NAME}...\")\n",
    "airnow_data = AirNowData(\n",
    "    start_date=START_DATE,\n",
    "    end_date=end_date_adj,\n",
    "    extent=extent,\n",
    "    airnow_api_key=os.getenv('AIRNOW_API_KEY'),\n",
    "    frames_per_sample=frames_per_sample,\n",
    "    dim=dim,\n",
    "    elevation_path=\"../libs/inputs/elevation.npy\",\n",
    "    mask_path=\"../libs/inputs/mask.npy\",\n",
    "    use_mask=False,\n",
    "    force_reprocess=False\n",
    ")\n",
    "X_airnow = airnow_data.data\n",
    "Y_targets = airnow_data.target_stations\n",
    "print(f\"✓ AirNow data shape: {X_airnow.shape}\")\n",
    "if Y_targets is not None:\n",
    "    print(f\"  Target stations shape: {Y_targets.shape}\")\n",
    "    print(f\"  Number of sensors: {len(airnow_data.air_sens_loc)}\")\n",
    "    print(f\"  Sensor names: {airnow_data.sensor_names}\")\n",
    "else:\n",
    "    print(\"  No target stations available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "create_combined_dataset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating combined dataset for January 2022 to December 2024...\n",
      "Data shapes before combining:\n",
      "  PWWB: (17540, 5, 40, 40, 4)\n",
      "  AirNow: (17540, 5, 40, 40, 1)\n",
      "Minimum timesteps across all datasets: 17535\n",
      "Data shapes after trimming:\n",
      "  PWWB: (17535, 5, 40, 40, 4)\n",
      "  AirNow: (17535, 5, 40, 40, 1)\n",
      "  Y_targets: (17535, 5, 8)\n",
      "Combined data shape: (17535, 5, 40, 40, 5)\n",
      "Channel breakdown:\n",
      "  PWWB: 4 channels\n",
      "  AirNow: 1 channels\n",
      "  Total: 5 channels\n",
      "  Channel names: ['MAIAC_AOD', 'TROPOMI_NO2', 'METAR_Wind_U', 'METAR_Wind_V', 'AirNow_PM25']\n"
     ]
    }
   ],
   "source": [
    "del airnow_data\n",
    "del pwwb_data\n",
    "\n",
    "# ========== 4. Create Combined Dataset ==========\n",
    "print(f\"\\nCreating combined dataset for {EXPERIMENT_NAME}...\")\n",
    "\n",
    "# First, check shapes and find minimum timesteps\n",
    "print(f\"Data shapes before combining:\")\n",
    "print(f\"  PWWB: {X_pwwb.shape}\")\n",
    "print(f\"  AirNow: {X_airnow.shape}\")\n",
    "\n",
    "# Find the minimum number of timesteps across all datasets\n",
    "min_timesteps = min(X_pwwb.shape[0], X_airnow.shape[0], Y_targets.shape[0])\n",
    "print(f\"Minimum timesteps across all datasets: {min_timesteps}\")\n",
    "\n",
    "# Trim all datasets to the same number of timesteps\n",
    "X_pwwb = X_pwwb[:min_timesteps]\n",
    "X_airnow = X_airnow[:min_timesteps]\n",
    "Y_targets = Y_targets[:min_timesteps]  # Don't forget to trim Y_targets too!\n",
    "\n",
    "print(f\"Data shapes after trimming:\")\n",
    "print(f\"  PWWB: {X_pwwb.shape}\")\n",
    "print(f\"  AirNow: {X_airnow.shape}\")\n",
    "print(f\"  Y_targets: {Y_targets.shape}\")\n",
    "\n",
    "# Combine all data sources\n",
    "X_combined = np.concatenate([X_pwwb, X_airnow], axis=-1)\n",
    "print(f\"Combined data shape: {X_combined.shape}\")\n",
    "\n",
    "# Display the number of channels from each source\n",
    "pwwb_channels = X_pwwb.shape[4]\n",
    "airnow_channels = X_airnow.shape[4]\n",
    "total_channels = X_combined.shape[4]\n",
    "\n",
    "print(f\"Channel breakdown:\")\n",
    "print(f\"  PWWB: {pwwb_channels} channels\")\n",
    "print(f\"  AirNow: {airnow_channels} channels\")\n",
    "print(f\"  Total: {total_channels} channels\")\n",
    "\n",
    "# Create combined channel names\n",
    "all_channel_names = channel_info['channel_names'] + [\"AirNow_PM25\"]\n",
    "print(f\"  Channel names: {all_channel_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "data_splits",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating temporal train/test split for January 2022 to December 2024...\n",
      "Temporal split at indices 12274 and 14904:\n",
      "  Training: samples 0-12273 (70% of time)\n",
      "  Validation: samples 12274-14903 (15% of time)\n",
      "  Testing: samples 14904-17534 (15% of time)\n",
      "✓ Train shapes: X=(12274, 5, 40, 40), Y=(12274, 5, 8)\n",
      "✓ Test shapes: X=(2631, 5, 40, 40), Y=(2631, 5, 8)\n",
      "✓ Valid shapes: X=(2630, 5, 40, 40), Y=(2630, 5, 8)\n",
      "✓ Scaled train shape: (12274, 5, 40, 40)\n",
      "✓ Scaled test shape: (2631, 5, 40, 40)\n",
      "✓ Scaled valid shape: (2630, 5, 40, 40)\n",
      "Saved MAIAC_AOD data.\n",
      "\n",
      "Temporal split at indices 12274 and 14904:\n",
      "  Training: samples 0-12273 (70% of time)\n",
      "  Validation: samples 12274-14903 (15% of time)\n",
      "  Testing: samples 14904-17534 (15% of time)\n",
      "✓ Train shapes: X=(12274, 5, 40, 40), Y=(12274, 5, 8)\n",
      "✓ Test shapes: X=(2631, 5, 40, 40), Y=(2631, 5, 8)\n",
      "✓ Valid shapes: X=(2630, 5, 40, 40), Y=(2630, 5, 8)\n",
      "✓ Scaled train shape: (12274, 5, 40, 40)\n",
      "✓ Scaled test shape: (2631, 5, 40, 40)\n",
      "✓ Scaled valid shape: (2630, 5, 40, 40)\n",
      "Saved TROPOMI_NO2 data.\n",
      "\n",
      "Temporal split at indices 12274 and 14904:\n",
      "  Training: samples 0-12273 (70% of time)\n",
      "  Validation: samples 12274-14903 (15% of time)\n",
      "  Testing: samples 14904-17534 (15% of time)\n",
      "✓ Train shapes: X=(12274, 5, 40, 40), Y=(12274, 5, 8)\n",
      "✓ Test shapes: X=(2631, 5, 40, 40), Y=(2631, 5, 8)\n",
      "✓ Valid shapes: X=(2630, 5, 40, 40), Y=(2630, 5, 8)\n",
      "✓ Scaled train shape: (12274, 5, 40, 40)\n",
      "✓ Scaled test shape: (2631, 5, 40, 40)\n",
      "✓ Scaled valid shape: (2630, 5, 40, 40)\n",
      "Saved METAR_Wind_U data.\n",
      "\n",
      "Temporal split at indices 12274 and 14904:\n",
      "  Training: samples 0-12273 (70% of time)\n",
      "  Validation: samples 12274-14903 (15% of time)\n",
      "  Testing: samples 14904-17534 (15% of time)\n",
      "✓ Train shapes: X=(12274, 5, 40, 40), Y=(12274, 5, 8)\n",
      "✓ Test shapes: X=(2631, 5, 40, 40), Y=(2631, 5, 8)\n",
      "✓ Valid shapes: X=(2630, 5, 40, 40), Y=(2630, 5, 8)\n",
      "✓ Scaled train shape: (12274, 5, 40, 40)\n",
      "✓ Scaled test shape: (2631, 5, 40, 40)\n",
      "✓ Scaled valid shape: (2630, 5, 40, 40)\n",
      "Saved METAR_Wind_V data.\n",
      "\n",
      "Temporal split at indices 12274 and 14904:\n",
      "  Training: samples 0-12273 (70% of time)\n",
      "  Validation: samples 12274-14903 (15% of time)\n",
      "  Testing: samples 14904-17534 (15% of time)\n",
      "✓ Train shapes: X=(12274, 5, 40, 40), Y=(12274, 5, 8)\n",
      "✓ Test shapes: X=(2631, 5, 40, 40), Y=(2631, 5, 8)\n",
      "✓ Valid shapes: X=(2630, 5, 40, 40), Y=(2630, 5, 8)\n",
      "✓ Scaled train shape: (12274, 5, 40, 40)\n",
      "✓ Scaled test shape: (2631, 5, 40, 40)\n",
      "✓ Scaled valid shape: (2630, 5, 40, 40)\n",
      "Saved AirNow_PM25 data.\n",
      "\n",
      "Y_train range: 0.00 to 279.70\n",
      "Y_train mean: 9.05, std: 7.12\n"
     ]
    }
   ],
   "source": [
    "del X_pwwb\n",
    "del X_airnow\n",
    "\n",
    "print(f\"\\nCreating temporal train/test split for {EXPERIMENT_NAME}...\")\n",
    "\n",
    "for i, c in enumerate(all_channel_names):\n",
    "    X_train, X_test, X_valid, Y_train, Y_test, Y_valid = temporal_train_test_valid_split(\n",
    "        X_combined[:,:,:,:,i], Y_targets, train_size=0.7, test_size=0.15, valid_size=0.15\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Train shapes: X={X_train.shape}, Y={Y_train.shape}\")\n",
    "    print(f\"✓ Test shapes: X={X_test.shape}, Y={Y_test.shape}\")\n",
    "    print(f\"✓ Valid shapes: X={X_valid.shape}, Y={Y_valid.shape}\")\n",
    "    \n",
    "    # ========== 6. Standardize Data ==========\n",
    "    X_train_scaled, X_test_scaled, X_valid_scaled = std_scale_train_test_valid(X_train, X_test, X_valid)\n",
    "    print(f\"✓ Scaled train shape: {X_train_scaled.shape}\")\n",
    "    print(f\"✓ Scaled test shape: {X_test_scaled.shape}\")\n",
    "    print(f\"✓ Scaled valid shape: {X_valid_scaled.shape}\")\n",
    "\n",
    "    np.save(f\"final_input_data/{all_channel_names[i]}_X_train.npy\", X_train_scaled)\n",
    "    np.save(f\"final_input_data/{all_channel_names[i]}_X_test.npy\", X_test_scaled)\n",
    "    np.save(f\"final_input_data/{all_channel_names[i]}_X_valid.npy\", X_valid_scaled)\n",
    "\n",
    "    print(f\"Saved {all_channel_names[i]} data.\\n\")\n",
    "    \n",
    "print(f\"Y_train range: {Y_train.min():.2f} to {Y_train.max():.2f}\")\n",
    "print(f\"Y_train mean: {Y_train.mean():.2f}, std: {Y_train.std():.2f}\")\n",
    "np.save(f\"final_input_data/Y_train.npy\", Y_train)\n",
    "np.save(f\"final_input_data/Y_test.npy\", Y_test)\n",
    "np.save(f\"final_input_data/Y_valid.npy\", Y_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
