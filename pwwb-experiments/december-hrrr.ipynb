{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deca3910-8b85-4d35-ba11-4278b6cfa502",
   "metadata": {},
   "source": [
    "# Experiment 2: No-fire December set + HRRR\n",
    "\n",
    "This notebook focuses on running Experiment 2, which uses data from December (no-fire period) combining PWWB, AirNow, and HRRR datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c92945-842a-4338-9ec2-a954bca054a8",
   "metadata": {},
   "source": [
    "# Data parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a247a5-a0e5-46f7-adbd-3234c2f952d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define bounding box\n",
    "lat_bottom, lat_top = 33.5, 34.5\n",
    "lon_bottom, lon_top = -118.75, -117.0\n",
    "extent = (lon_bottom, lon_top, lat_bottom, lat_top)\n",
    "\n",
    "# input data shape\n",
    "dim = 200\n",
    "frames_per_sample = 5\n",
    "\n",
    "# date range of data - two weeks of December for this experiment 2024-12-08-00\", \"2024-12-21-00\"\n",
    "dec_start_date, dec_end_date = \"2024-12-01-00\", \"2024-12-31-00\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a17cf45-39ee-4320-a2b6-93bf9c095667",
   "metadata": {},
   "source": [
    "# Data ingestion and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28cd894d-b165-4b11-9d41-0608bdf6783e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moh/miniconda3/envs/hysplitrevised/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# python nonsense that allows you to import from sibling directories\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import the new PWWB implementation and dataset manager\n",
    "from libs.pwwb import PWWBData\n",
    "from libs.pwwb.utils.dataset_manager import create_dataset_manager\n",
    "\n",
    "# Import the AirNow data class\n",
    "from libs.airnowdata import AirNowData\n",
    "\n",
    "# Import the HRRR data class\n",
    "from libs.hrrrdata import HRRRData\n",
    "\n",
    "# Load environment variables (API keys, credentials)\n",
    "load_dotenv()\n",
    "\n",
    "# split data\n",
    "def train_test_split(X, train_size=0.75):\n",
    "    split_idx = int(X.shape[0] * train_size)\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n",
    "# scale training data, then scale test data based on training data stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def std_scale(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_train = scaler.fit_transform(X_train.reshape(-1, 1)).reshape(X_train.shape)\n",
    "    scaled_test = scaler.transform(X_test.reshape(-1, 1)).reshape(X_test.shape)\n",
    "\n",
    "    return scaled_train, scaled_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "setup_dataset_manager",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing datasets:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>created</th>\n",
       "      <th>description</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>channels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dec2024_MAIC_TROPOMI_N02_METAR_WIND</td>\n",
       "      <td>2025-05-20T17:58:39.575630</td>\n",
       "      <td>December 2024 - month with MAIAC, TROPOMI NO2,...</td>\n",
       "      <td>2024-12-01-00</td>\n",
       "      <td>2024-12-3-00</td>\n",
       "      <td>maiac, tropomi, metar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dec2024_MAIC_TROPOMI_N02_METAR_WIND_UV_ONLY</td>\n",
       "      <td>2025-05-20T18:34:46.342993</td>\n",
       "      <td>December 2024 - month with MAIAC, TROPOMI NO2,...</td>\n",
       "      <td>2024-12-01-00</td>\n",
       "      <td>2024-12-3-00</td>\n",
       "      <td>maiac, tropomi, metar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          name                     created  \\\n",
       "0          dec2024_MAIC_TROPOMI_N02_METAR_WIND  2025-05-20T17:58:39.575630   \n",
       "1  dec2024_MAIC_TROPOMI_N02_METAR_WIND_UV_ONLY  2025-05-20T18:34:46.342993   \n",
       "\n",
       "                                         description     start_date  \\\n",
       "0  December 2024 - month with MAIAC, TROPOMI NO2,...  2024-12-01-00   \n",
       "1  December 2024 - month with MAIAC, TROPOMI NO2,...  2024-12-01-00   \n",
       "\n",
       "       end_date               channels  \n",
       "0  2024-12-3-00  maiac, tropomi, metar  \n",
       "1  2024-12-3-00  maiac, tropomi, metar  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create output directory for results\n",
    "output_dir = \"experiment_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create dataset manager\n",
    "manager = create_dataset_manager(\n",
    "    registry_file=\"experiment2_registry.json\",\n",
    "    cache_dir=\"data/pwwb_cache/\"\n",
    ")\n",
    "\n",
    "# List existing datasets\n",
    "print(\"Existing datasets:\")\n",
    "try:\n",
    "    display(manager.list_datasets())\n",
    "except:\n",
    "    print(\"No existing datasets found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process_dec_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading December PWWB data...\n",
      "Dataset 'dec2024_MAIC_TROPOMI_N02_METAR_WIND_UV_ONLY' already exists, loading from cache...\n",
      "Using cache prefix: dec2024_MAIC_TROPOMI_N02_METAR_WIND_UV_ONLY_\n",
      "Initialized PWWBData with 48 hourly timestamps\n",
      "Date range: 2024-12-01 00:00:00 to 2024-12-02 23:00:00\n",
      "Channels included: ['maiac', 'tropomi', 'metar']\n",
      "TROPOMI channels: ['TROPOMI_NO2']\n",
      "METAR channels: ['METAR_Wind_U', 'METAR_Wind_V']\n",
      "Processing MAIAC AOD data...\n",
      "Loading cached MAIAC AOD data from data/pwwb_cache/dec2024_MAIC_TROPOMI_N02_METAR_WIND_UV_ONLY_maiac_aod_data.npy\n",
      "Processing TROPOMI data...\n",
      "Including TROPOMI channels: ['TROPOMI_NO2']\n",
      "Loading cached TROPOMI data from data/pwwb_cache/dec2024_MAIC_TROPOMI_N02_METAR_WIND_UV_ONLY_tropomi_no2_data.npy\n",
      "Processing METAR meteorological data...\n",
      "Initialized MetarDataSource with 2 channels: ['METAR_Wind_U', 'METAR_Wind_V']\n",
      "Will fetch these raw variables: ['sped', 'drct']\n",
      "Will calculate wind U/V components from speed/direction\n",
      "Loading cached data from data/pwwb_cache/dec2024_MAIC_TROPOMI_N02_METAR_WIND_UV_ONLY_metar_u_v_data.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moh/wildfire/hrrr-smoke-viz/pwwb-experiments/../libs/pwwb/pwwb_data.py:115: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  self.timestamps = pd.date_range(self.start_date, self.end_date, freq='H')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final data shape: (44, 5, 200, 200, 4)\n",
      "\n",
      "Channel Statistics:\n",
      "===================\n",
      "\n",
      "Channel 0: MAIAC_AOD\n",
      "  Min: -28672.000000000007\n",
      "  Max: -11408.21815610717\n",
      "  Mean: -22674.333462043887\n",
      "  Std: 5129.642805682386\n",
      "  Data coverage: 100.00% (40000/40000 non-zero pixels)\n",
      "\n",
      "Channel 1: TROPOMI_NO2\n",
      "  Min: 0.00012060817200200961\n",
      "  Max: 0.0005489974885201102\n",
      "  Mean: 0.00029664643713431493\n",
      "  Std: 8.174618747433876e-05\n",
      "  Data coverage: 100.00% (40000/40000 non-zero pixels)\n",
      "\n",
      "Channel 2: METAR_Wind_U\n",
      "  Min: 5.75\n",
      "  Max: 6.483879083422768\n",
      "  Mean: 6.086833897749474\n",
      "  Std: 0.0759926512972382\n",
      "  Data coverage: 100.00% (40000/40000 non-zero pixels)\n",
      "\n",
      "Channel 3: METAR_Wind_V\n",
      "  Min: 0.0\n",
      "  Max: 2.359938988947114\n",
      "  Mean: 0.06280266341815492\n",
      "  Std: 0.27971648397314036\n",
      "  Data coverage: 6.71% (2685/40000 non-zero pixels)\n",
      "\n",
      "Final Data Shape:\n",
      "  44 samples\n",
      "  5 frames per sample\n",
      "  200x200 grid size\n",
      "  4 channels\n",
      "\n",
      "Data Memory Usage:\n",
      "  268.55 MB\n",
      "Data file data/pwwb_cache/dec2024_MAIC_TROPOMI_N02_METAR_WIND_UV_ONLY_full_data.npy does not exist. Cannot load data.\n",
      "Warning: Created PWWBData instance but could not load cached data.\n",
      "✓ December PWWB data shape: (44, 5, 200, 200, 4)\n",
      "  Channels: ['MAIAC_AOD', 'TROPOMI_NO2', 'METAR_Wind_U', 'METAR_Wind_V']\n",
      "\n",
      "Loading December AirNow data...\n",
      "Using mask from ../libs/inputs/mask.npy\n",
      "Loading processed AirNow data from cache: data/airnow_processed.npz\n",
      "✓ Successfully loaded processed data from cache\n",
      "  - Data shape: (44, 5, 200, 200, 1)\n",
      "  - Found 14 sensor locations\n",
      "✓ December AirNow data shape: (44, 5, 200, 200, 1)\n",
      "  December target stations shape: (44, 14)\n",
      "\n",
      "Loading December HRRR data...\n"
     ]
    }
   ],
   "source": [
    "# Adjust end date for AirNow\n",
    "dec_end_date_adj = pd.to_datetime(dec_end_date) - pd.Timedelta(hours=1)\n",
    "\n",
    "# Dataset name and description\n",
    "dataset_name = \"dec2024_MAIC_TROPOMI_N02_METAR_WIND_UV_ONLY\"\n",
    "dataset_desc = \"December 2024 - two weeks with MAIAC, TROPOMI NO2, METAR Wind U/V components only\"\n",
    "\n",
    "# ========== 1. Load December PWWB Data ==========\n",
    "print(\"\\nLoading December PWWB data...\")\n",
    "\n",
    "# Check if dataset already exists in the registry\n",
    "if manager.get_dataset_info(dataset_name) is not None:\n",
    "    print(f\"Dataset '{dataset_name}' already exists, loading from cache...\")\n",
    "    dec_pwwb = manager.load_dataset(dataset_name, PWWBData)\n",
    "else:\n",
    "    print(f\"Dataset '{dataset_name}' not found, creating new one...\")\n",
    "    # Create the dataset with the specified channels\n",
    "    dec_pwwb = manager.create_dataset(\n",
    "        name=dataset_name,\n",
    "        description=dataset_desc,\n",
    "        PWWBData_class=PWWBData,\n",
    "        start_date=dec_start_date,\n",
    "        end_date=dec_end_date,\n",
    "        extent=extent,\n",
    "        frames_per_sample=frames_per_sample,\n",
    "        dim=dim,\n",
    "        include_channels={\n",
    "            'maiac': True,                     # Include MAIAC AOD\n",
    "            'tropomi': ['TROPOMI_NO2'],        # Only include NO2 from TROPOMI\n",
    "            'metar': ['METAR_Wind_U', 'METAR_Wind_V'],  # Only wind components from METAR\n",
    "            'modis_fire': False,               # Exclude MODIS fire data\n",
    "            'merra2': False                    # Exclude MERRA2 data\n",
    "        },\n",
    "        verbose=True,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    # Save the dataset\n",
    "    dec_pwwb.save_data()\n",
    "\n",
    "# Get the data and channel info\n",
    "X_dec_pwwb = dec_pwwb.data\n",
    "channel_info = dec_pwwb.get_channel_info()\n",
    "print(f\"✓ December PWWB data shape: {X_dec_pwwb.shape}\")\n",
    "print(f\"  Channels: {channel_info['channel_names']}\")\n",
    "\n",
    "# ========== 2. Load December AirNow Data ==========\n",
    "print(\"\\nLoading December AirNow data...\")\n",
    "dec_airnow = AirNowData(\n",
    "    start_date=dec_start_date,\n",
    "    end_date=dec_end_date_adj,\n",
    "    extent=extent,\n",
    "    airnow_api_key=os.getenv('AIRNOW_API_KEY'),\n",
    "    frames_per_sample=frames_per_sample,\n",
    "    dim=dim,\n",
    "    elevation_path=\"../libs/inputs/elevation.npy\",\n",
    "    mask_path=\"../libs/inputs/mask.npy\",\n",
    "    force_reprocess=False\n",
    ")\n",
    "X_dec_airnow = dec_airnow.data\n",
    "Y_dec = dec_airnow.target_stations\n",
    "print(f\"✓ December AirNow data shape: {X_dec_airnow.shape}\")\n",
    "if Y_dec is not None:\n",
    "    print(f\"  December target stations shape: {Y_dec.shape}\")\n",
    "else:\n",
    "    print(\"  No December target stations available\")\n",
    "\n",
    "# ========== 3. Load December HRRR Data ==========\n",
    "print(\"\\nLoading December HRRR data...\")\n",
    "dec_hrrr = HRRRData(\n",
    "    start_date=dec_start_date,\n",
    "    end_date=dec_end_date_adj,\n",
    "    extent=extent,\n",
    "    extent_name='la_region',\n",
    "    product='COLMD',\n",
    "    frames_per_sample=frames_per_sample,\n",
    "    dim=dim,\n",
    "    verbose=True,\n",
    "    sample_setting=2\n",
    ")\n",
    "# Convert units from kg to micrograms (1e9)\n",
    "X_dec_hrrr = dec_hrrr.data\n",
    "print(f\"✓ December HRRR data shape: {X_dec_hrrr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "create_experiments",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Experiment 2 dataset...\n",
      "  Experiment 2: No-fire December set + HRRR\n",
      "    Combined shape: (44, 5, 200, 200, 6)\n",
      "    PWWB channels: 4, AirNow channels: 1, HRRR channels: 1, Total: 6\n"
     ]
    }
   ],
   "source": [
    "# ========== 4. Create Experiment 2 dataset ==========\n",
    "print(\"\\nCreating Experiment 2 dataset...\")\n",
    "\n",
    "# Experiment 2: No-fire December set + HRRR\n",
    "print(\"  Experiment 2: No-fire December set + HRRR\")\n",
    "X_exp2 = np.concatenate([X_dec_pwwb, X_dec_airnow, X_dec_hrrr], axis=-1)\n",
    "print(f\"    Combined shape: {X_exp2.shape}\")\n",
    "\n",
    "# Display the number of channels from each source\n",
    "pwwb_channels = X_dec_pwwb.shape[4]\n",
    "airnow_channels = X_dec_airnow.shape[4]\n",
    "hrrr_channels = X_dec_hrrr.shape[4]\n",
    "print(f\"    PWWB channels: {pwwb_channels}, AirNow channels: {airnow_channels}, HRRR channels: {hrrr_channels}, Total: {X_exp2.shape[4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "data_splits",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating train/test splits for experiment...\n",
      "  Experiment 2: Train=(33, 5, 200, 200, 6), Test=(11, 5, 200, 200, 6)\n",
      "\n",
      "Standardizing data...\n",
      "  Experiment 2: Scaled train=(33, 5, 200, 200, 6), test=(11, 5, 200, 200, 6)\n"
     ]
    }
   ],
   "source": [
    "# ========== 5. Train/Test Split for experiment ==========\n",
    "print(\"\\nCreating train/test splits for experiment...\")\n",
    "# Experiment 2 splits\n",
    "X_exp2_train, X_exp2_test = train_test_split(X_exp2, train_size=0.75)\n",
    "Y_dec_train, Y_dec_test = train_test_split(Y_dec, train_size=0.75)\n",
    "print(f\"  Experiment 2: Train={X_exp2_train.shape}, Test={X_exp2_test.shape}\")\n",
    "\n",
    "# ========== 6. Standardize data ==========\n",
    "print(\"\\nStandardizing data...\")\n",
    "\n",
    "# Experiment 2 standardization\n",
    "X_exp2_train_scaled, X_exp2_test_scaled = std_scale(X_exp2_train, X_exp2_test)\n",
    "print(f\"  Experiment 2: Scaled train={X_exp2_train_scaled.shape}, test={X_exp2_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "save_datasets",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving prepared dataset...\n",
      "\n",
      "✓ Dataset prepared and saved!\n"
     ]
    }
   ],
   "source": [
    "# ========== 7. Save prepared datasets ==========\n",
    "print(\"\\nSaving prepared dataset...\")\n",
    "\n",
    "# Create directory for experiment\n",
    "exp_dir = os.path.join(output_dir, \"experiment2\")\n",
    "os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "# Save Experiment 2 data\n",
    "np.save(os.path.join(exp_dir, \"X_train.npy\"), X_exp2_train_scaled)\n",
    "np.save(os.path.join(exp_dir, \"X_test.npy\"), X_exp2_test_scaled)\n",
    "np.save(os.path.join(exp_dir, \"y_train.npy\"), Y_dec_train)\n",
    "np.save(os.path.join(exp_dir, \"y_test.npy\"), Y_dec_test)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    \"date_range\": f\"{dec_start_date} to {dec_end_date}\",\n",
    "    \"extent\": extent,\n",
    "    \"dim\": dim,\n",
    "    \"frames_per_sample\": frames_per_sample,\n",
    "    \"pwwb_channels\": channel_info['channel_names'],\n",
    "    \"airnow_channels\": [\"AirNow_PM25\"],\n",
    "    \"hrrr_channels\": [\"HRRR_COLMD\"]\n",
    "}\n",
    "\n",
    "with open(os.path.join(exp_dir, \"metadata.json\"), \"w\") as f:\n",
    "    import json\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"\\n✓ Dataset prepared and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6812d262-11a1-4927-926e-9ad0682975f9",
   "metadata": {},
   "source": [
    "# Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf1eba7-2cbd-485a-a9da-1c46ba2d01d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize data from experiment\n",
    "def visualize_experiment_data(X, y, channel_names=None, sample_idx=None):\n",
    "    \"\"\"Visualize data from the experiment\"\"\"\n",
    "    # Get a random sample if none provided\n",
    "    if sample_idx is None:\n",
    "        np.random.seed(42)\n",
    "        sample_idx = np.random.choice(range(len(X)), size=1)[0]\n",
    "    \n",
    "    # Get channel information\n",
    "    n_channels = X.shape[4]\n",
    "    n_frames = X.shape[1]\n",
    "    \n",
    "    # Use provided channel names or create default ones\n",
    "    if channel_names is None or len(channel_names) != n_channels:\n",
    "        channel_names = [f\"Channel {i}\" for i in range(n_channels)]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(n_channels, n_frames, figsize=(3*n_frames, 2*n_channels))\n",
    "    if n_channels == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    # Plot each channel and frame\n",
    "    for c in range(n_channels):\n",
    "        for f in range(n_frames):\n",
    "            ax = axes[c, f]\n",
    "            im = ax.imshow(X[sample_idx, f, :, :, c])\n",
    "            plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "            if f == 0:\n",
    "                ax.set_ylabel(channel_names[c])\n",
    "            ax.set_title(f\"Frame {f+1}\")\n",
    "    \n",
    "    # Set title\n",
    "    plt.suptitle(f\"Experiment 2: No-fire December set + HRRR\\nSample {sample_idx}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print target values\n",
    "    if y is not None:\n",
    "        print(f\"Target values: {y[sample_idx]}\")\n",
    "\n",
    "# Create combined channel names list\n",
    "all_channel_names = channel_info['channel_names'] + [\"AirNow_PM25\"] + [\"HRRR_COLMD\"]\n",
    "\n",
    "# Visualize a sample from the experiment\n",
    "print(\"Visualizing data...\")\n",
    "visualize_experiment_data(X_exp2_train_scaled, Y_dec_train, channel_names=all_channel_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df24d06-dd0d-43e5-9882-433592c04ac3",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c4bb57-54cf-4ee5-8805-a25b32b79da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv3D\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Convolution2D, MaxPooling3D, Flatten, Reshape\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import InputLayer\n",
    "\n",
    "tf.keras.backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b8f7aa-b358-4828-9d23-dcfe919ed971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Experiment 2: No-fire December set + HRRR\n",
    "print(\"\\n==== Running Experiment 2: No-fire December set + HRRR ====\")\n",
    "print(f\"Training data shape: {X_exp2_train_scaled.shape}\")\n",
    "print(f\"Target data shape: {Y_dec_train.shape}\")\n",
    "\n",
    "# Build model\n",
    "seq = Sequential()\n",
    "\n",
    "seq.add(\n",
    "    InputLayer(shape=X_exp2_train_scaled.shape[1:])\n",
    ")\n",
    "\n",
    "seq.add(\n",
    "    ConvLSTM2D(\n",
    "            filters=15, \n",
    "            kernel_size=(3, 3),\n",
    "            padding='same', \n",
    "            return_sequences=True\n",
    "    )\n",
    ")\n",
    "\n",
    "seq.add(\n",
    "    ConvLSTM2D(\n",
    "        filters=30, \n",
    "        kernel_size=(3, 3),\n",
    "        padding='same', \n",
    "        return_sequences=True\n",
    "    )\n",
    ")\n",
    "\n",
    "seq.add(\n",
    "    Conv3D(\n",
    "        filters=15, \n",
    "        kernel_size=(3, 3, 3),\n",
    "        activation='relu',\n",
    "        padding='same'    \n",
    "    )\n",
    ")\n",
    "\n",
    "seq.add(\n",
    "    Conv3D(\n",
    "        filters=1, \n",
    "        kernel_size=(3, 3, 3),\n",
    "        activation='relu',\n",
    "        padding='same'\n",
    "    )\n",
    ")\n",
    "\n",
    "seq.add(Flatten())\n",
    "seq.add(Dense(Y_dec_train.shape[1], activation='relu'))\n",
    "\n",
    "# Compile model\n",
    "seq.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "\n",
    "# Print model summary\n",
    "seq.summary()\n",
    "\n",
    "# Train model\n",
    "print(f\"\\nTraining model...\")\n",
    "epochs = 100  # Set to 100 to match Experiment 1\n",
    "batch_size = 4\n",
    "history = seq.fit(\n",
    "    X_exp2_train_scaled, Y_dec_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "print(f\"\\nEvaluating model...\")\n",
    "test_loss = seq.evaluate(X_exp2_test_scaled, Y_dec_test, verbose=0)\n",
    "print(f\"Test MAE: {test_loss:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = seq.predict(X_exp2_test_scaled, verbose=0)\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "mae = mean_absolute_error(Y_dec_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(Y_dec_test, y_pred))\n",
    "r2 = r2_score(Y_dec_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "if 'val_loss' in history.history:\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.legend()\n",
    "plt.title('Experiment 2: No-fire December set + HRRR\\nTraining Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MAE)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "results_dir = os.path.join(output_dir, \"experiment2\", \"results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "np.save(os.path.join(results_dir, \"y_pred.npy\"), y_pred)\n",
    "seq.save(os.path.join(results_dir, \"model.h5\"))\n",
    "\n",
    "exp2_results = {\n",
    "    'model': seq,\n",
    "    'history': history,\n",
    "    'loss': test_loss,\n",
    "    'mae': mae,\n",
    "    'rmse': rmse,\n",
    "    'r2': r2,\n",
    "    'y_pred': y_pred,\n",
    "    'y_test': Y_dec_test\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a464ca9a-7130-4c6e-a342-5e8c5a9baa11",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa443727-40fc-46e0-9310-e39639290d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nDetailed analysis for Experiment 2:\")\n",
    "X_test = X_exp2_test_scaled\n",
    "y_test = Y_dec_test\n",
    "y_pred = exp2_results['y_pred']\n",
    "model = exp2_results['model']\n",
    "description = \"No-fire December set + HRRR, two weeks\"\n",
    "\n",
    "print(f\"Analyzing Experiment 2: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b3905-4697-441f-a66f-635d8c90221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.plotting import (\n",
    "    plot_prediction_comparison,\n",
    "    plot_scatter_comparison,\n",
    "    plot_error_by_sensor,\n",
    "    plot_time_series_comparison,\n",
    "    plot_input_frames,\n",
    "    print_metrics\n",
    ")\n",
    "\n",
    "# Sensor names (use AirNow sensor names if available)\n",
    "if hasattr(dec_airnow, 'sensor_names') and dec_airnow.sensor_names is not None:\n",
    "    sensor_names = dec_airnow.sensor_names\n",
    "else:\n",
    "    sensor_names = [\n",
    "        \"North Holywood\", \n",
    "        \"Los Angeles - N. Main Street\", \n",
    "        \"Compton\",\n",
    "        \"Crestline - Lake Gregory\",\n",
    "        \"Fontana - Arrow Highway\",\n",
    "        \"Glendora - Laurel\",\n",
    "        \"Lake Elsinore - W. Flint Street\",\n",
    "        \"Long Beach Signal Hill\",\n",
    "        \"Mira Loma - Van Buren\",\n",
    "        \"Reseda\",\n",
    "        \"Riverside - Rubidoux\",\n",
    "        \"Santa Clarita\",\n",
    "        \"Simi Valley - Cochran Street\",\n",
    "        \"Temecula (Lake Skinner)\"\n",
    "    ]\n",
    "\n",
    "print(\"\\n1. Plotting prediction comparison...\")\n",
    "plot_prediction_comparison(y_pred, y_test, sensor_names, sample_idx=8)\n",
    "\n",
    "print(\"\\n2. Plotting scatter comparison...\")\n",
    "plot_scatter_comparison(y_pred, y_test)\n",
    "\n",
    "print(\"\\n3. Plotting error by sensor...\")\n",
    "plot_error_by_sensor(y_pred, y_test, sensor_names)\n",
    "\n",
    "print(\"\\n4. Plotting time series comparison...\")\n",
    "plot_time_series_comparison(y_pred, y_test, sensor_names)\n",
    "    \n",
    "print(\"\\n5. Plotting time series with shifted predictions...\")\n",
    "plot_time_series_comparison(y_pred, y_test, sensor_names, shift_pred=1)\n",
    "\n",
    "print(\"\\n6. Printing metrics...\")\n",
    "print_metrics(y_pred, y_test, sensor_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_exp_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save experiment comparison\n",
    "with open(os.path.join(output_dir, 'experiment2_results.txt'), 'w') as f:\n",
    "    f.write(\"==== Experiment 2 Results ====\\n\")\n",
    "    f.write(f\"Experiment 2 (No-fire December + HRRR, two weeks): MAE = {exp2_results['mae']:.4f}, RMSE = {exp2_results['rmse']:.4f}, R² = {exp2_results['r2']:.4f}\\n\")\n",
    "    f.write(f\"\\nAnalysis completed on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(\"\\nExperiment 2 complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hysplitrevised",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
