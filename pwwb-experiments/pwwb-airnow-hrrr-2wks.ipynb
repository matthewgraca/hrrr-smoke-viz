{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deca3910-8b85-4d35-ba11-4278b6cfa502",
   "metadata": {},
   "source": [
    "# Usage\n",
    "Ideally the only thing that needs to be changed is: \n",
    "- The start/end date of the data\n",
    "- The data ingestion portion\n",
    "\n",
    "The rest should be taken care of, assuming no bugs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c92945-842a-4338-9ec2-a954bca054a8",
   "metadata": {},
   "source": [
    "# Data parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a247a5-a0e5-46f7-adbd-3234c2f952d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define bounding box\n",
    "lat_bottom, lat_top = 33.9, 34.2\n",
    "lon_bottom, lon_top = -118.4, -118.0\n",
    "extent = (lon_bottom, lon_top, lat_bottom, lat_top)\n",
    "\n",
    "# input data shape\n",
    "dim = 200\n",
    "frames_per_sample = 5\n",
    "\n",
    "# date range of data - MODIFIED to use second week of December and January\n",
    "dec_start_date, dec_end_date = \"2024-12-16-00\", \"2024-12-23-00\"\n",
    "jan_start_date, jan_end_date = \"2025-01-16-00\", \"2025-01-23-00\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a17cf45-39ee-4320-a2b6-93bf9c095667",
   "metadata": {},
   "source": [
    "# Data ingestion and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28cd894d-b165-4b11-9d41-0608bdf6783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python nonsense that allows you to import from sibling directories\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import the data classes\n",
    "from libs.pwwbdata import PWWBData\n",
    "from libs.airnowdata import AirNowData\n",
    "from libs.hrrrdata import HRRRData\n",
    "\n",
    "# Load environment variables (API keys, credentials)\n",
    "load_dotenv()\n",
    "\n",
    "# split data\n",
    "def train_test_split(X, train_size=0.75):\n",
    "    split_idx = int(X.shape[0] * train_size)\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n",
    "# scale training data, then scale test data based on training data stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def std_scale(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_train = scaler.fit_transform(X_train.reshape(-1, 1)).reshape(X_train.shape)\n",
    "    scaled_test = scaler.transform(X_test.reshape(-1, 1)).reshape(X_test.shape)\n",
    "\n",
    "    return scaled_train, scaled_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process_dec_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading December PWWB data...\n",
      "Initialized PWWBData with 168 hourly timestamps\n",
      "Date range: 2024-12-16 00:00:00 to 2024-12-22 23:00:00\n",
      "Initialized PWWBData with 168 hourly timestamps\n",
      "Date range: 2024-12-16 00:00:00 to 2024-12-22 23:00:00\n",
      "Processing remote-sensing satellite imagery...\n",
      "Fetching MAIAC AOD data for 7 unique dates\n",
      "AOD data shape before resize: (5, 1200, 1200)\n",
      "AOD data type: int16\n",
      "AOD data min/max: -28672/537\n",
      "Averaged AOD data to shape: (1200, 1200)\n",
      "Resized AOD data to shape: (200, 200)\n",
      "Successfully processed AOD data for 2024-12-16\n",
      "AOD data shape before resize: (4, 1200, 1200)\n",
      "AOD data type: int16\n",
      "AOD data min/max: -28672/564\n",
      "Averaged AOD data to shape: (1200, 1200)\n",
      "Resized AOD data to shape: (200, 200)\n",
      "Successfully processed AOD data for 2024-12-17\n",
      "AOD data shape before resize: (4, 1200, 1200)\n",
      "AOD data type: int16\n",
      "AOD data min/max: -28672/554\n",
      "Averaged AOD data to shape: (1200, 1200)\n",
      "Resized AOD data to shape: (200, 200)\n",
      "Successfully processed AOD data for 2024-12-18\n",
      "AOD data shape before resize: (4, 1200, 1200)\n",
      "AOD data type: int16\n",
      "AOD data min/max: -28672/485\n",
      "Averaged AOD data to shape: (1200, 1200)\n",
      "Resized AOD data to shape: (200, 200)\n",
      "Successfully processed AOD data for 2024-12-19\n",
      "AOD data shape before resize: (4, 1200, 1200)\n",
      "AOD data type: int16\n",
      "AOD data min/max: -28672/793\n",
      "Averaged AOD data to shape: (1200, 1200)\n",
      "Resized AOD data to shape: (200, 200)\n",
      "Successfully processed AOD data for 2024-12-20\n",
      "AOD data shape before resize: (5, 1200, 1200)\n",
      "AOD data type: int16\n",
      "AOD data min/max: -28672/957\n",
      "Averaged AOD data to shape: (1200, 1200)\n",
      "Resized AOD data to shape: (200, 200)\n",
      "Successfully processed AOD data for 2024-12-21\n",
      "AOD data shape before resize: (4, 1200, 1200)\n",
      "AOD data type: int16\n",
      "AOD data min/max: -28672/609\n",
      "Averaged AOD data to shape: (1200, 1200)\n",
      "Resized AOD data to shape: (200, 200)\n",
      "Successfully processed AOD data for 2024-12-22\n",
      "Created MAIAC AOD data with shape (168, 200, 200, 1)\n",
      "Fetching TROPOMI data for 7 unique dates\n",
      "Processing TROPOMI data for date: 2024-12-16\n",
      "Successfully processed NO2 data\n",
      "Successfully processed CH4 data\n",
      "Successfully processed CO data\n",
      "Processing TROPOMI data for date: 2024-12-17\n",
      "Successfully processed NO2 data\n",
      "Successfully processed CH4 data\n",
      "Successfully processed CO data\n",
      "Processing TROPOMI data for date: 2024-12-18\n",
      "Successfully processed NO2 data\n",
      "Successfully processed CH4 data\n",
      "Successfully processed CO data\n",
      "Processing TROPOMI data for date: 2024-12-19\n",
      "Successfully processed NO2 data\n",
      "Successfully processed CH4 data\n",
      "Successfully processed CO data\n",
      "Processing TROPOMI data for date: 2024-12-20\n",
      "Successfully processed NO2 data\n",
      "Successfully processed CH4 data\n",
      "Successfully processed CO data\n",
      "Processing TROPOMI data for date: 2024-12-21\n",
      "Successfully processed NO2 data\n",
      "Successfully processed CH4 data\n",
      "Successfully processed CO data\n",
      "Processing TROPOMI data for date: 2024-12-22\n",
      "Successfully processed NO2 data\n",
      "Successfully processed CH4 data\n",
      "Successfully processed CO data\n",
      "Created TROPOMI data with shape (168, 200, 200, 3)\n",
      "Processing wildfire/smoke data...\n",
      "Fetching MODIS fire data for 7 unique dates\n",
      "Processing MODIS fire data for date: 2024-12-16\n",
      "Found LST data with shape: (1200, 1200)\n",
      "Data type: uint16\n",
      "Min value: 0, Max value: 15185\n",
      "Successfully processed LST data for 2024-12-16\n",
      "Processing MODIS fire data for date: 2024-12-17\n",
      "Found LST data with shape: (1200, 1200)\n",
      "Data type: uint16\n",
      "Min value: 0, Max value: 15241\n",
      "Successfully processed LST data for 2024-12-17\n",
      "Processing MODIS fire data for date: 2024-12-18\n",
      "Found LST data with shape: (1200, 1200)\n",
      "Data type: uint16\n",
      "Min value: 0, Max value: 15244\n",
      "Successfully processed LST data for 2024-12-18\n",
      "Processing MODIS fire data for date: 2024-12-19\n",
      "Found LST data with shape: (1200, 1200)\n",
      "Data type: uint16\n",
      "Min value: 0, Max value: 16992\n",
      "Successfully processed LST data for 2024-12-19\n",
      "Processing MODIS fire data for date: 2024-12-20\n",
      "Found LST data with shape: (1200, 1200)\n",
      "Data type: uint16\n",
      "Min value: 0, Max value: 15223\n",
      "Successfully processed LST data for 2024-12-20\n",
      "Processing MODIS fire data for date: 2024-12-21\n",
      "Found LST data with shape: (1200, 1200)\n",
      "Data type: uint16\n",
      "Min value: 0, Max value: 15259\n",
      "Successfully processed LST data for 2024-12-21\n",
      "Processing MODIS fire data for date: 2024-12-22\n",
      "Found LST data with shape: (1200, 1200)\n",
      "Data type: uint16\n",
      "Min value: 0, Max value: 15110\n",
      "Successfully processed LST data for 2024-12-22\n",
      "Created MODIS fire data with shape (168, 200, 200, 1)\n",
      "Fetching MERRA-2 data for period: 2024-12-16 00:00:00 to 2024-12-22 23:00:00\n",
      "Processing MERRA-2 data for period: 2024-12-16 to 2024-12-22\n",
      "Found 7 MERRA-2 granules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUEUEING TASKS | : 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 945.15it/s]\n",
      "PROCESSING TASKS | :   0%|                                                                                                                                                                   | 0/7 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Create output directory for results\n",
    "output_dir = \"experiment_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "dec_end_date_adj = pd.to_datetime(dec_end_date) - pd.Timedelta(hours=1)\n",
    "# ========== 1. Load December PWWB Data ==========\n",
    "print(\"\\nLoading December PWWB data...\")\n",
    "dec_pwwb = PWWBData(\n",
    "    start_date=dec_start_date,\n",
    "    end_date=dec_end_date,\n",
    "    extent=extent,\n",
    "    frames_per_sample=frames_per_sample,\n",
    "    dim=dim,\n",
    "    env_file='.env',\n",
    "    verbose=True,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "X_dec_pwwb = dec_pwwb.data\n",
    "channel_info = dec_pwwb.get_channel_info()\n",
    "print(f\"✓ December PWWB data shape: {X_dec_pwwb.shape}\")\n",
    "print(f\"  Channels: {channel_info['channel_order']}\")\n",
    "\n",
    "# ========== 2. Load December AirNow Data ==========\n",
    "print(\"\\nLoading December AirNow data...\")\n",
    "dec_airnow = AirNowData(\n",
    "    start_date=dec_start_date,\n",
    "    end_date=dec_end_date_adj,\n",
    "    extent=extent,\n",
    "    airnow_api_key=os.getenv('AIRNOW_API_KEY'),\n",
    "    frames_per_sample=frames_per_sample,\n",
    "    dim=dim,\n",
    "    elevation_path=\"../libs/inputs/elevation.npy\",\n",
    "    mask_path=\"../libs/inputs/mask.npy\",\n",
    "    force_reprocess=False\n",
    ")\n",
    "X_dec_airnow = dec_airnow.data\n",
    "Y_dec = dec_airnow.target_stations\n",
    "print(f\"✓ December AirNow data shape: {X_dec_airnow.shape}\")\n",
    "if Y_dec is not None:\n",
    "    print(f\"  December target stations shape: {Y_dec.shape}\")\n",
    "else:\n",
    "    print(\"  No December target stations available\")\n",
    "\n",
    "# ========== 3. Load December HRRR Data ==========\n",
    "print(\"\\nLoading December HRRR data...\")\n",
    "dec_hrrr = HRRRData(\n",
    "    start_date=dec_start_date,\n",
    "    end_date=dec_end_date_adj,\n",
    "    extent=extent,\n",
    "    extent_name='la_region',\n",
    "    product='COLMD',\n",
    "    frames_per_sample=frames_per_sample,\n",
    "    dim=dim,\n",
    "    verbose=True,\n",
    "    sample_setting=2\n",
    ")\n",
    "# Convert units from kg to micrograms (1e9)\n",
    "X_dec_hrrr = dec_hrrr.data\n",
    "print(f\"✓ December HRRR data shape: {X_dec_hrrr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process_jan_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_end_date_adj = pd.to_datetime(jan_end_date) - pd.Timedelta(hours=1)\n",
    "# ========== 4. Load January PWWB Data ==========\n",
    "print(\"\\nLoading January PWWB data...\")\n",
    "jan_pwwb = PWWBData(\n",
    "    start_date=jan_start_date,\n",
    "    end_date=jan_end_date,\n",
    "    extent=extent,\n",
    "    frames_per_sample=frames_per_sample,\n",
    "    dim=dim,\n",
    "    env_file='.env',\n",
    "    verbose=True,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "X_jan_pwwb = jan_pwwb.data\n",
    "print(f\"✓ January PWWB data shape: {X_jan_pwwb.shape}\")\n",
    "\n",
    "# ========== 5. Load January AirNow Data ==========\n",
    "print(\"\\nLoading January AirNow data...\")\n",
    "jan_airnow = AirNowData(\n",
    "    start_date=jan_start_date,\n",
    "    end_date=jan_end_date_adj,\n",
    "    extent=extent,\n",
    "    airnow_api_key=os.getenv('AIRNOW_API_KEY'),\n",
    "    frames_per_sample=frames_per_sample,\n",
    "    dim=dim,\n",
    "    elevation_path=\"../libs/inputs/elevation.npy\",\n",
    "    mask_path=\"../libs/inputs/mask.npy\",\n",
    "    force_reprocess=False\n",
    ")\n",
    "X_jan_airnow = jan_airnow.data\n",
    "Y_jan = jan_airnow.target_stations\n",
    "print(f\"✓ January AirNow data shape: {X_jan_airnow.shape}\")\n",
    "if Y_jan is not None:\n",
    "    print(f\"  January target stations shape: {Y_jan.shape}\")\n",
    "else:\n",
    "    print(\"  No January target stations available\")\n",
    "\n",
    "# ========== 6. Load January HRRR Data ==========\n",
    "print(\"\\nLoading January HRRR data...\")\n",
    "jan_hrrr = HRRRData(\n",
    "    start_date=jan_start_date,\n",
    "    end_date=jan_end_date_adj,\n",
    "    extent=extent,\n",
    "    extent_name='la_region',\n",
    "    product='COLMD',\n",
    "    frames_per_sample=frames_per_sample,\n",
    "    dim=dim,\n",
    "    verbose=True,\n",
    "    sample_setting=2\n",
    ")\n",
    "X_jan_hrrr = jan_hrrr.data\n",
    "print(f\"✓ January HRRR data shape: {X_jan_hrrr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_experiments",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 7. Create the four experiment datasets ==========\n",
    "print(\"\\nCreating the four experiment datasets...\")\n",
    "\n",
    "# Experiment 1: No-fire December set (PWWB + AirNow)\n",
    "print(\"  Experiment 1: No-fire December set (PWWB + AirNow)\")\n",
    "X_exp1 = np.concatenate([X_dec_pwwb, X_dec_airnow], axis=-1)\n",
    "print(f\"    Combined shape: {X_exp1.shape}\")\n",
    "\n",
    "# # Experiment 2: No-fire December set + HRRR\n",
    "# print(\"  Experiment 2: No-fire December set + HRRR\")\n",
    "# X_exp2 = np.concatenate([X_dec_pwwb, X_dec_airnow, X_dec_hrrr], axis=-1)\n",
    "# print(f\"    Combined shape: {X_exp2.shape}\")\n",
    "\n",
    "# # Experiment 3: Fire January set (PWWB + AirNow)\n",
    "# print(\"  Experiment 3: Fire January set (PWWB + AirNow)\")\n",
    "# X_exp3 = np.concatenate([X_jan_pwwb, X_jan_airnow], axis=-1)\n",
    "# print(f\"    Combined shape: {X_exp3.shape}\")\n",
    "\n",
    "# # Experiment 4: Fire January set + HRRR\n",
    "# print(\"  Experiment 4: Fire January set + HRRR\")\n",
    "# X_exp4 = np.concatenate([X_jan_pwwb, X_jan_airnow, X_jan_hrrr], axis=-1)\n",
    "# print(f\"    Combined shape: {X_exp4.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_splits",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 8. Train/Test Split for each experiment ==========\n",
    "print(\"\\nCreating train/test splits for each experiment...\")\n",
    "\n",
    "# Experiment 1 splits\n",
    "X_exp1_train, X_exp1_test = train_test_split(X_exp1, train_size=0.75)\n",
    "Y_dec_train, Y_dec_test = train_test_split(Y_dec, train_size=0.75)\n",
    "print(f\"  Experiment 1: Train={X_exp1_train.shape}, Test={X_exp1_test.shape}\")\n",
    "\n",
    "# # Experiment 2 splits\n",
    "# X_exp2_train, X_exp2_test = train_test_split(X_exp2, train_size=0.75)\n",
    "# print(f\"  Experiment 2: Train={X_exp2_train.shape}, Test={X_exp2_test.shape}\")\n",
    "\n",
    "# # Experiment 3 splits\n",
    "# X_exp3_train, X_exp3_test = train_test_split(X_exp3, train_size=0.75)\n",
    "# Y_jan_train, Y_jan_test = train_test_split(Y_jan, train_size=0.75)\n",
    "# print(f\"  Experiment 3: Train={X_exp3_train.shape}, Test={X_exp3_test.shape}\")\n",
    "\n",
    "# # Experiment 4 splits\n",
    "# X_exp4_train, X_exp4_test = train_test_split(X_exp4, train_size=0.75)\n",
    "# print(f\"  Experiment 4: Train={X_exp4_train.shape}, Test={X_exp4_test.shape}\")\n",
    "\n",
    "# ========== 9. Standardize data for each experiment ==========\n",
    "print(\"\\nStandardizing data for each experiment...\")\n",
    "\n",
    "# Experiment 1 standardization\n",
    "X_exp1_train_scaled, X_exp1_test_scaled = std_scale(X_exp1_train, X_exp1_test)\n",
    "print(f\"  Experiment 1: Scaled train={X_exp1_train_scaled.shape}, test={X_exp1_test_scaled.shape}\")\n",
    "\n",
    "# # Experiment 2 standardization\n",
    "# X_exp2_train_scaled, X_exp2_test_scaled = std_scale(X_exp2_train, X_exp2_test)\n",
    "# print(f\"  Experiment 2: Scaled train={X_exp2_train_scaled.shape}, test={X_exp2_test_scaled.shape}\")\n",
    "\n",
    "# # Experiment 3 standardization\n",
    "# X_exp3_train_scaled, X_exp3_test_scaled = std_scale(X_exp3_train, X_exp3_test)\n",
    "# print(f\"  Experiment 3: Scaled train={X_exp3_train_scaled.shape}, test={X_exp3_test_scaled.shape}\")\n",
    "\n",
    "# # Experiment 4 standardization\n",
    "# X_exp4_train_scaled, X_exp4_test_scaled = std_scale(X_exp4_train, X_exp4_test)\n",
    "# print(f\"  Experiment 4: Scaled train={X_exp4_train_scaled.shape}, test={X_exp4_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_datasets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 10. Save prepared datasets ==========\n",
    "print(\"\\nSaving prepared datasets...\")\n",
    "\n",
    "# Create directories for each experiment\n",
    "exp_dirs = {}\n",
    "for i in range(1, 5):\n",
    "    exp_dir = os.path.join(output_dir, f\"experiment{i}\")\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    exp_dirs[i] = exp_dir\n",
    "\n",
    "# Save Experiment 1 data\n",
    "np.save(os.path.join(exp_dirs[1], \"X_train.npy\"), X_exp1_train_scaled)\n",
    "np.save(os.path.join(exp_dirs[1], \"X_test.npy\"), X_exp1_test_scaled)\n",
    "np.save(os.path.join(exp_dirs[1], \"y_train.npy\"), Y_dec_train)\n",
    "np.save(os.path.join(exp_dirs[1], \"y_test.npy\"), Y_dec_test)\n",
    "\n",
    "# # Save Experiment 2 data\n",
    "# np.save(os.path.join(exp_dirs[2], \"X_train.npy\"), X_exp2_train_scaled)\n",
    "# np.save(os.path.join(exp_dirs[2], \"X_test.npy\"), X_exp2_test_scaled)\n",
    "# np.save(os.path.join(exp_dirs[2], \"y_train.npy\"), Y_dec_train)\n",
    "# np.save(os.path.join(exp_dirs[2], \"y_test.npy\"), Y_dec_test)\n",
    "\n",
    "# # Save Experiment 3 data\n",
    "# np.save(os.path.join(exp_dirs[3], \"X_train.npy\"), X_exp3_train_scaled)\n",
    "# np.save(os.path.join(exp_dirs[3], \"X_test.npy\"), X_exp3_test_scaled)\n",
    "# np.save(os.path.join(exp_dirs[3], \"y_train.npy\"), Y_jan_train)\n",
    "# np.save(os.path.join(exp_dirs[3], \"y_test.npy\"), Y_jan_test)\n",
    "\n",
    "# # Save Experiment 4 data\n",
    "# np.save(os.path.join(exp_dirs[4], \"X_train.npy\"), X_exp4_train_scaled)\n",
    "# np.save(os.path.join(exp_dirs[4], \"X_test.npy\"), X_exp4_test_scaled)\n",
    "# np.save(os.path.join(exp_dirs[4], \"y_train.npy\"), Y_jan_train)\n",
    "# np.save(os.path.join(exp_dirs[4], \"y_test.npy\"), Y_jan_test)\n",
    "\n",
    "print(\"\\n✓ All datasets prepared and saved!\")\n",
    "print(\"Ready to run experiments 1-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6812d262-11a1-4927-926e-9ad0682975f9",
   "metadata": {},
   "source": [
    "# Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf1eba7-2cbd-485a-a9da-1c46ba2d01d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize data from any experiment\n",
    "def visualize_experiment_data(X, y, exp_number, sample_idx=None):\n",
    "    \"\"\"Visualize data from a specific experiment\"\"\"\n",
    "    # Get a random sample if none provided\n",
    "    if sample_idx is None:\n",
    "        np.random.seed(42)\n",
    "        sample_idx = np.random.choice(range(len(X)), size=1)[0]\n",
    "    \n",
    "    # Get channel information\n",
    "    n_channels = X.shape[4]\n",
    "    n_frames = X.shape[1]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(n_channels, n_frames, figsize=(3*n_frames, 2*n_channels))\n",
    "    if n_channels == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    # Plot each channel and frame\n",
    "    for c in range(n_channels):\n",
    "        for f in range(n_frames):\n",
    "            ax = axes[c, f]\n",
    "            ax.imshow(X[sample_idx, f, :, :, c])\n",
    "            if f == 0:\n",
    "                ax.set_ylabel(f\"Channel {c}\")\n",
    "            ax.set_title(f\"Frame {f+1}\")\n",
    "            ax.axis(\"off\")\n",
    "    \n",
    "    # Set title\n",
    "    exp_descriptions = {\n",
    "        1: \"No-fire December set (PWWB + AirNow)\",\n",
    "        # 2: \"No-fire December set + HRRR\",\n",
    "        # 3: \"Fire January set (PWWB + AirNow)\",\n",
    "        # 4: \"Fire January set + HRRR\"\n",
    "    }\n",
    "    plt.suptitle(f\"Experiment {exp_number}: {exp_descriptions[exp_number]}\\nSample {sample_idx}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print target values\n",
    "    print(f\"Target values: {y[sample_idx]}\")\n",
    "\n",
    "# Visualize a sample from each experiment\n",
    "print(\"Visualizing data from all experiments...\")\n",
    "visualize_experiment_data(X_exp1_train_scaled, Y_dec_train, 1)\n",
    "# visualize_experiment_data(X_exp2_train_scaled, Y_dec_train, 2)\n",
    "# visualize_experiment_data(X_exp3_train_scaled, Y_jan_train, 3)\n",
    "# visualize_experiment_data(X_exp4_train_scaled, Y_jan_train, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df24d06-dd0d-43e5-9882-433592c04ac3",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c4bb57-54cf-4ee5-8805-a25b32b79da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv3D\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Convolution2D, MaxPooling3D, Flatten, Reshape\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import InputLayer\n",
    "\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "# Function to run a single experiment\n",
    "def run_experiment(exp_number, epochs=150, batch_size=4):\n",
    "    \"\"\"Train and evaluate a model for a specific experiment\"\"\"\n",
    "    # Set up experiment data\n",
    "    if exp_number == 1:\n",
    "        X_train = X_exp1_train_scaled\n",
    "        y_train = Y_dec_train\n",
    "        X_test = X_exp1_test_scaled\n",
    "        y_test = Y_dec_test\n",
    "        description = \"No-fire December set (PWWB + AirNow)\"\n",
    "    # elif exp_number == 2:\n",
    "    #     X_train = X_exp2_train_scaled\n",
    "    #     y_train = Y_dec_train\n",
    "    #     X_test = X_exp2_test_scaled\n",
    "    #     y_test = Y_dec_test\n",
    "    #     description = \"No-fire December set + HRRR\"\n",
    "    # elif exp_number == 3:\n",
    "    #     X_train = X_exp3_train_scaled\n",
    "    #     y_train = Y_jan_train\n",
    "    #     X_test = X_exp3_test_scaled\n",
    "    #     y_test = Y_jan_test\n",
    "    #     description = \"Fire January set (PWWB + AirNow)\"\n",
    "    # elif exp_number == 4:\n",
    "    #     X_train = X_exp4_train_scaled\n",
    "    #     y_train = Y_jan_train\n",
    "    #     X_test = X_exp4_test_scaled\n",
    "    #     y_test = Y_jan_test\n",
    "    #     description = \"Fire January set + HRRR\"\n",
    "    else:\n",
    "        print(f\"Invalid experiment number: {exp_number}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n==== Running Experiment {exp_number}: {description} ====\")\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Target data shape: {y_train.shape}\")\n",
    "    \n",
    "    # Build model\n",
    "    seq = Sequential()\n",
    "    \n",
    "    seq.add(\n",
    "        InputLayer(shape=X_train.shape[1:])\n",
    "    )\n",
    "    \n",
    "    seq.add(\n",
    "        ConvLSTM2D(\n",
    "                filters=15, \n",
    "                kernel_size=(3, 3),\n",
    "                padding='same', \n",
    "                return_sequences=True\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    seq.add(\n",
    "        ConvLSTM2D(\n",
    "            filters=30, \n",
    "            kernel_size=(3, 3),\n",
    "            padding='same', \n",
    "            return_sequences=True\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    seq.add(\n",
    "        Conv3D(\n",
    "            filters=15, \n",
    "            kernel_size=(3, 3, 3),\n",
    "            activation='relu',\n",
    "            padding='same'    \n",
    "        )\n",
    "    )\n",
    "    \n",
    "    seq.add(\n",
    "        Conv3D(\n",
    "            filters=1, \n",
    "            kernel_size=(3, 3, 3),\n",
    "            activation='relu',\n",
    "            padding='same'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    seq.add(Flatten())\n",
    "    seq.add(Dense(y_train.shape[1], activation='relu'))\n",
    "    \n",
    "    # Compile model\n",
    "    seq.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    \n",
    "    # Print model summary\n",
    "    seq.summary()\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\nTraining model for experiment {exp_number}...\")\n",
    "    history = seq.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(f\"\\nEvaluating model for experiment {exp_number}...\")\n",
    "    test_loss = seq.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test MAE: {test_loss:.4f}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = seq.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.legend()\n",
    "    plt.title(f'Experiment {exp_number}: {description}\\nTraining Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (MAE)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Save results\n",
    "    results_dir = os.path.join(output_dir, f\"experiment{exp_number}\", \"results\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    np.save(os.path.join(results_dir, \"y_pred.npy\"), y_pred)\n",
    "    seq.save(os.path.join(results_dir, \"model.h5\"))\n",
    "    \n",
    "    return {\n",
    "        'model': seq,\n",
    "        'history': history,\n",
    "        'loss': test_loss,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'y_pred': y_pred,\n",
    "        'y_test': y_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b8f7aa-b358-4828-9d23-dcfe919ed971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Experiment 1: No-fire December set (PWWB + AirNow)\n",
    "exp1_results = run_experiment(1, epochs=50)  # Reduced epochs for faster testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_exp2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Experiment 2: No-fire December set + HRRR\n",
    "# exp2_results = run_experiment(2, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_exp3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Experiment 3: Fire January set (PWWB + AirNow)\n",
    "# exp3_results = run_experiment(3, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_exp4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Experiment 4: Fire January set + HRRR\n",
    "# exp4_results = run_experiment(4, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a464ca9a-7130-4c6e-a342-5e8c5a9baa11",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results across all experiments\n",
    "print(\"\\n==== Experiment Comparison ====\")\n",
    "print(f\"Experiment 1 (No-fire December): MAE = {exp1_results['mae']:.4f}\")\n",
    "# print(f\"Experiment 2 (No-fire December + HRRR): MAE = {exp2_results['mae']:.4f}\")\n",
    "# print(f\"Experiment 3 (Fire January): MAE = {exp3_results['mae']:.4f}\")\n",
    "# print(f\"Experiment 4 (Fire January + HRRR): MAE = {exp4_results['mae']:.4f}\")\n",
    "\n",
    "# Visualize comparison\n",
    "experiments = ['Exp 1\\nDec (2nd week)', 'Exp 2\\nDec+HRRR', 'Exp 3\\nJan (2nd week)', 'Exp 4\\nJan+HRRR']\n",
    "# , exp2_results['mae'], exp3_results['mae'], exp4_results['mae']\n",
    "maes = [exp1_results['mae']]\n",
    "# ,exp2_results['rmse'], exp3_results['rmse'], exp4_results['rmse']\n",
    "rmses = [exp1_results['rmse']]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot MAEs\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(experiments, maes, color=['blue', 'skyblue', 'red', 'salmon'])\n",
    "plt.title('MAE Comparison')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(maes):\n",
    "    plt.text(i, v + 0.01, f'{v:.4f}', ha='center')\n",
    "\n",
    "# Plot RMSEs\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(experiments, rmses, color=['blue', 'skyblue', 'red', 'salmon'])\n",
    "plt.title('RMSE Comparison')\n",
    "plt.ylabel('Root Mean Squared Error')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(rmses):\n",
    "    plt.text(i, v + 0.01, f'{v:.4f}', ha='center')\n",
    "\n",
    "plt.suptitle('Performance Comparison Across Experiments')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'experiment_comparison.png'))\n",
    "plt.show()\n",
    "\n",
    "# Save comparison results\n",
    "with open(os.path.join(output_dir, 'experiment_comparison.txt'), 'w') as f:\n",
    "    f.write(\"==== Experiment Comparison ====\\n\")\n",
    "    f.write(f\"Experiment 1 (No-fire December, 2nd week): MAE = {exp1_results['mae']:.4f}, RMSE = {exp1_results['rmse']:.4f}, R² = {exp1_results['r2']:.4f}\\n\")\n",
    "    # f.write(f\"Experiment 2 (No-fire December + HRRR, 2nd week): MAE = {exp2_results['mae']:.4f}, RMSE = {exp2_results['rmse']:.4f}, R² = {exp2_results['r2']:.4f}\\n\")\n",
    "    # f.write(f\"Experiment 3 (Fire January, 2nd week): MAE = {exp3_results['mae']:.4f}, RMSE = {exp3_results['rmse']:.4f}, R² = {exp3_results['r2']:.4f}\\n\")\n",
    "    # f.write(f\"Experiment 4 (Fire January + HRRR, 2nd week): MAE = {exp4_results['mae']:.4f}, RMSE = {exp4_results['rmse']:.4f}, R² = {exp4_results['r2']:.4f}\\n\")\n",
    "    f.write(f\"\\nAnalysis completed on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(\"\\nExperiment comparison complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa443727-40fc-46e0-9310-e39639290d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For detailed analysis of a specific experiment (e.g., Experiment 1)\n",
    "# Choose which experiment to analyze\n",
    "selected_exp = 1  # Change this to analyze a different experiment (1-4)\n",
    "\n",
    "# Set the appropriate data and results\n",
    "if selected_exp == 1:\n",
    "    X_test = X_exp1_test_scaled\n",
    "    y_test = Y_dec_test\n",
    "    y_pred = exp1_results['y_pred']\n",
    "    model = exp1_results['model']\n",
    "    description = \"No-fire December set (PWWB + AirNow), 2nd week\"\n",
    "# elif selected_exp == 2:\n",
    "#     X_test = X_exp2_test_scaled\n",
    "#     y_test = Y_dec_test\n",
    "#     y_pred = exp2_results['y_pred']\n",
    "#     model = exp2_results['model']\n",
    "#     description = \"No-fire December set + HRRR, 2nd week\"\n",
    "# elif selected_exp == 3:\n",
    "#     X_test = X_exp3_test_scaled\n",
    "#     y_test = Y_jan_test\n",
    "#     y_pred = exp3_results['y_pred']\n",
    "#     model = exp3_results['model']\n",
    "#     description = \"Fire January set (PWWB + AirNow), 2nd week\"\n",
    "# elif selected_exp == 4:\n",
    "#     X_test = X_exp4_test_scaled\n",
    "#     y_test = Y_jan_test\n",
    "#     y_pred = exp4_results['y_pred']\n",
    "#     model = exp4_results['model']\n",
    "#     description = \"Fire January set + HRRR, 2nd week\"\n",
    "else:\n",
    "    print(f\"Invalid experiment number: {selected_exp}\")\n",
    "\n",
    "print(f\"Analyzing Experiment {selected_exp}: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b3905-4697-441f-a66f-635d8c90221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.plotting import (\n",
    "    plot_prediction_comparison,\n",
    "    plot_scatter_comparison,\n",
    "    plot_error_by_sensor,\n",
    "    plot_time_series_comparison,\n",
    "    plot_input_frames,\n",
    "    print_metrics\n",
    ")\n",
    "\n",
    "# Sensor names (use AirNow sensor names if available)\n",
    "if hasattr(dec_airnow, 'sensor_names') and dec_airnow.sensor_names is not None:\n",
    "    sensor_names = dec_airnow.sensor_names\n",
    "else:\n",
    "    sensor_names = [\"North Hollywood\", \"Los Angeles - N. Main Street\", \"Compton\"]\n",
    "\n",
    "print(f\"\\nDetailed analysis for Experiment {selected_exp}:\")\n",
    "print(\"\\n1. Plotting prediction comparison...\")\n",
    "plot_prediction_comparison(y_pred, y_test, sensor_names, sample_idx=12)\n",
    "\n",
    "print(\"\\n2. Plotting scatter comparison...\")\n",
    "plot_scatter_comparison(y_pred, y_test)\n",
    "\n",
    "print(\"\\n3. Plotting error by sensor...\")\n",
    "plot_error_by_sensor(y_pred, y_test, sensor_names)\n",
    "\n",
    "print(\"\\n4. Plotting time series comparison...\")\n",
    "plot_time_series_comparison(y_pred, y_test, sensor_names)\n",
    "    \n",
    "print(\"\\n5. Plotting time series with shifted predictions...\")\n",
    "plot_time_series_comparison(y_pred, y_test, sensor_names, shift_pred=1)\n",
    "\n",
    "print(\"\\n6. Printing metrics...\")\n",
    "print_metrics(y_pred, y_test, sensor_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
