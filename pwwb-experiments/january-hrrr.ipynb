{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deca3910-8b85-4d35-ba11-4278b6cfa502",
   "metadata": {},
   "source": [
    "# Experiment 4: fire January set + HRRR\n",
    "\n",
    "This notebook focuses on running Experiment 4, which uses data from January (no-fire period) combining PWWB, AirNow, and HRRR datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c92945-842a-4338-9ec2-a954bca054a8",
   "metadata": {},
   "source": [
    "# Data parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a247a5-a0e5-46f7-adbd-3234c2f952d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define bounding box\n",
    "lat_bottom, lat_top = 33.5, 34.5\n",
    "lon_bottom, lon_top = -118.75, -117.0\n",
    "extent = (lon_bottom, lon_top, lat_bottom, lat_top)\n",
    "\n",
    "# input data shape\n",
    "dim = 200\n",
    "frames_per_sample = 5\n",
    " \n",
    "# date range of data - two weeks of January for this Experiment 4024-12-08-00\", \"2024-12-21-00\"\n",
    "jan_start_date, jan_end_date = \"2025-01-01-00\", \"2025-02-01-00\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a17cf45-39ee-4320-a2b6-93bf9c095667",
   "metadata": {},
   "source": [
    "# Data ingestion and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cd894d-b165-4b11-9d41-0608bdf6783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python nonsense that allows you to import from sibling directories\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import the new PWWB implementation and dataset manager\n",
    "from libs.pwwb import PWWBData\n",
    "from libs.pwwb.utils.dataset_manager import create_dataset_manager\n",
    "\n",
    "# Import the AirNow data class\n",
    "from libs.airnowdata import AirNowData\n",
    "\n",
    "# Import the HRRR data class\n",
    "from libs.hrrrdata import HRRRData\n",
    "\n",
    "# Load environment variables (API keys, credentials)\n",
    "load_dotenv()\n",
    "\n",
    "# split data\n",
    "def train_test_split(X, train_size=0.75):\n",
    "    split_idx = int(X.shape[0] * train_size)\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n",
    "# scale training data, then scale test data based on training data stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def std_scale(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_train = scaler.fit_transform(X_train.reshape(-1, 1)).reshape(X_train.shape)\n",
    "    scaled_test = scaler.transform(X_test.reshape(-1, 1)).reshape(X_test.shape)\n",
    "\n",
    "    return scaled_train, scaled_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_dataset_manager",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for results\n",
    "output_dir = \"experiment_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create dataset manager\n",
    "manager = create_dataset_manager(\n",
    "    registry_file=\"experiment4_registry.json\",\n",
    "    cache_dir=\"data/pwwb_cache/\"\n",
    ")\n",
    "\n",
    "# List existing datasets\n",
    "print(\"Existing datasets:\")\n",
    "try:\n",
    "    display(manager.list_datasets())\n",
    "except:\n",
    "    print(\"No existing datasets found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process_jan_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust end date for AirNow\n",
    "jan_end_date_adj = pd.to_datetime(jan_end_date) - pd.Timedelta(hours=1)\n",
    "\n",
    "# Dataset name and description\n",
    "dataset_name = \"jan2024_MAIC_TROPOMI_N02_METAR_WIND_UV_ONLY\"\n",
    "dataset_desc = \"January 2024 - two weeks with MAIAC, TROPOMI NO2, METAR Wind U/V components only\"\n",
    "\n",
    "# ========== 1. Load January PWWB Data ==========\n",
    "print(\"\\nLoading January PWWB data...\")\n",
    "\n",
    "# Check if dataset already exists in the registry\n",
    "if manager.get_dataset_info(dataset_name) is not None:\n",
    "    print(f\"Dataset '{dataset_name}' already exists, loading from cache...\")\n",
    "    jan_pwwb = manager.load_dataset(dataset_name, PWWBData)\n",
    "else:\n",
    "    print(f\"Dataset '{dataset_name}' not found, creating new one...\")\n",
    "    # Create the dataset with the specified channels\n",
    "    jan_pwwb = manager.create_dataset(\n",
    "        name=dataset_name,\n",
    "        description=dataset_desc,\n",
    "        PWWBData_class=PWWBData,\n",
    "        start_date=jan_start_date,\n",
    "        end_date=jan_end_date,\n",
    "        extent=extent,\n",
    "        frames_per_sample=frames_per_sample,\n",
    "        dim=dim,\n",
    "        include_channels={\n",
    "            'maiac': True,                     # Include MAIAC AOD\n",
    "            'tropomi': ['TROPOMI_NO2'],        # Only include NO2 from TROPOMI\n",
    "            'metar': ['METAR_Wind_U', 'METAR_Wind_V'],  # Only wind components from METAR\n",
    "            'modis_fire': False,               # Exclude MODIS fire data\n",
    "            'merra2': False                    # Exclude MERRA2 data\n",
    "        },\n",
    "        verbose=True,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    # Save the dataset\n",
    "    jan_pwwb.save_data()\n",
    "\n",
    "# Get the data and channel info\n",
    "X_jan_pwwb = jan_pwwb.data\n",
    "channel_info = jan_pwwb.get_channel_info()\n",
    "print(f\"✓ January PWWB data shape: {X_jan_pwwb.shape}\")\n",
    "print(f\"  Channels: {channel_info['channel_names']}\")\n",
    "\n",
    "# ========== 2. Load January AirNow Data ==========\n",
    "print(\"\\nLoading January AirNow data...\")\n",
    "jan_airnow = AirNowData(\n",
    "    start_date=jan_start_date,\n",
    "    end_date=jan_end_date_adj,\n",
    "    extent=extent,\n",
    "    airnow_api_key=os.getenv('AIRNOW_API_KEY'),\n",
    "    frames_per_sample=frames_per_sample,\n",
    "    dim=dim,\n",
    "    elevation_path=\"../libs/inputs/elevation.npy\",\n",
    "    mask_path=\"../libs/inputs/mask.npy\",\n",
    "    force_reprocess=False\n",
    ")\n",
    "X_jan_airnow = jan_airnow.data\n",
    "Y_jan = jan_airnow.target_stations\n",
    "print(f\"✓ January AirNow data shape: {X_jan_airnow.shape}\")\n",
    "if Y_jan is not None:\n",
    "    print(f\"  January target stations shape: {Y_jan.shape}\")\n",
    "else:\n",
    "    print(\"  No January target stations available\")\n",
    "\n",
    "# ========== 3. Load January HRRR Data ==========\n",
    "print(\"\\nLoading January HRRR data...\")\n",
    "jan_hrrr = HRRRData(\n",
    "    start_date=jan_start_date,\n",
    "    end_date=jan_end_date,\n",
    "    extent=extent,\n",
    "    extent_name='la_region',\n",
    "    product='COLMD',\n",
    "    frames_per_sample=frames_per_sample,\n",
    "    dim=dim,\n",
    "    verbose=True,\n",
    "    sample_setting=2\n",
    ")\n",
    "# Convert units from kg to micrograms (1e9)\n",
    "X_jan_hrrr = jan_hrrr.data\n",
    "print(f\"✓ January HRRR data shape: {X_jan_hrrr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_experiments",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 4. Create Experiment 4 dataset ==========\n",
    "print(\"\\nCreating Experiment 4 dataset...\")\n",
    "\n",
    "# Experiment 4: No-fire January set + HRRR\n",
    "print(\"  Experiment 4: No-fire January set + HRRR\")\n",
    "X_exp2 = np.concatenate([X_jan_pwwb, X_jan_airnow, X_jan_hrrr], axis=-1)\n",
    "print(f\"    Combined shape: {X_exp2.shape}\")\n",
    "\n",
    "# Display the number of channels from each source\n",
    "pwwb_channels = X_jan_pwwb.shape[4]\n",
    "airnow_channels = X_jan_airnow.shape[4]\n",
    "hrrr_channels = X_jan_hrrr.shape[4]\n",
    "print(f\"    PWWB channels: {pwwb_channels}, AirNow channels: {airnow_channels}, HRRR channels: {hrrr_channels}, Total: {X_exp2.shape[4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_splits",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 5. Train/Test Split for experiment ==========\n",
    "print(\"\\nCreating train/test splits for experiment...\")\n",
    "# Experiment 4 splits\n",
    "X_exp2_train, X_exp2_test = train_test_split(X_exp2, train_size=0.75)\n",
    "Y_jan_train, Y_jan_test = train_test_split(Y_jan, train_size=0.75)\n",
    "print(f\"  Experiment 4: Train={X_exp2_train.shape}, Test={X_exp2_test.shape}\")\n",
    "\n",
    "# ========== 6. Standardize data ==========\n",
    "print(\"\\nStandardizing data...\")\n",
    "\n",
    "# Experiment 4 standardization\n",
    "X_exp2_train_scaled, X_exp2_test_scaled = std_scale(X_exp2_train, X_exp2_test)\n",
    "print(f\"  Experiment 4: Scaled train={X_exp2_train_scaled.shape}, test={X_exp2_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_datasets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 7. Save prepared datasets ==========\n",
    "print(\"\\nSaving prepared dataset...\")\n",
    "\n",
    "# Create directory for experiment\n",
    "exp_dir = os.path.join(output_dir, \"experiment4\")\n",
    "os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "# Save Experiment 4 data\n",
    "np.save(os.path.join(exp_dir, \"X_train.npy\"), X_exp2_train_scaled)\n",
    "np.save(os.path.join(exp_dir, \"X_test.npy\"), X_exp2_test_scaled)\n",
    "np.save(os.path.join(exp_dir, \"y_train.npy\"), Y_jan_train)\n",
    "np.save(os.path.join(exp_dir, \"y_test.npy\"), Y_jan_test)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    \"date_range\": f\"{jan_start_date} to {jan_end_date}\",\n",
    "    \"extent\": extent,\n",
    "    \"dim\": dim,\n",
    "    \"frames_per_sample\": frames_per_sample,\n",
    "    \"pwwb_channels\": channel_info['channel_names'],\n",
    "    \"airnow_channels\": [\"AirNow_PM25\"],\n",
    "    \"hrrr_channels\": [\"HRRR_COLMD\"]\n",
    "}\n",
    "\n",
    "with open(os.path.join(exp_dir, \"metadata.json\"), \"w\") as f:\n",
    "    import json\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"\\n✓ Dataset prepared and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6812d262-11a1-4927-926e-9ad0682975f9",
   "metadata": {},
   "source": [
    "# Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf1eba7-2cbd-485a-a9da-1c46ba2d01d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize data from experiment\n",
    "def visualize_experiment_data(X, y, channel_names=None, sample_idx=None):\n",
    "    \"\"\"Visualize data from the experiment\"\"\"\n",
    "    # Get a random sample if none provided\n",
    "    if sample_idx is None:\n",
    "        np.random.seed(42)\n",
    "        sample_idx = np.random.choice(range(len(X)), size=1)[0]\n",
    "    \n",
    "    # Get channel information\n",
    "    n_channels = X.shape[4]\n",
    "    n_frames = X.shape[1]\n",
    "    \n",
    "    # Use provided channel names or create default ones\n",
    "    if channel_names is None or len(channel_names) != n_channels:\n",
    "        channel_names = [f\"Channel {i}\" for i in range(n_channels)]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(n_channels, n_frames, figsize=(3*n_frames, 2*n_channels))\n",
    "    if n_channels == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    # Plot each channel and frame\n",
    "    for c in range(n_channels):\n",
    "        for f in range(n_frames):\n",
    "            ax = axes[c, f]\n",
    "            im = ax.imshow(X[sample_idx, f, :, :, c])\n",
    "            plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "            if f == 0:\n",
    "                ax.set_ylabel(channel_names[c])\n",
    "            ax.set_title(f\"Frame {f+1}\")\n",
    "    \n",
    "    # Set title\n",
    "    plt.suptitle(f\"Experiment 4: No-fire January set + HRRR\\nSample {sample_idx}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print target values\n",
    "    if y is not None:\n",
    "        print(f\"Target values: {y[sample_idx]}\")\n",
    "\n",
    "# Create combined channel names list\n",
    "all_channel_names = channel_info['channel_names'] + [\"AirNow_PM25\"] + [\"HRRR_COLMD\"]\n",
    "\n",
    "# Visualize a sample from the experiment\n",
    "print(\"Visualizing data...\")\n",
    "visualize_experiment_data(X_exp2_train_scaled, Y_jan_train, channel_names=all_channel_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df24d06-dd0d-43e5-9882-433592c04ac3",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c4bb57-54cf-4ee5-8805-a25b32b79da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv3D\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Convolution2D, MaxPooling3D, Flatten, Reshape\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import InputLayer\n",
    "\n",
    "tf.keras.backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b8f7aa-b358-4828-9d23-dcfe919ed971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Experiment 4: No-fire January set + HRRR\n",
    "print(\"\\n==== Running Experiment 4: No-fire January set + HRRR ====\")\n",
    "print(f\"Training data shape: {X_exp2_train_scaled.shape}\")\n",
    "print(f\"Target data shape: {Y_jan_train.shape}\")\n",
    "\n",
    "# Build model\n",
    "seq = Sequential()\n",
    "\n",
    "seq.add(\n",
    "    InputLayer(shape=X_exp2_train_scaled.shape[1:])\n",
    ")\n",
    "\n",
    "seq.add(\n",
    "    ConvLSTM2D(\n",
    "            filters=15, \n",
    "            kernel_size=(3, 3),\n",
    "            padding='same', \n",
    "            return_sequences=True\n",
    "    )\n",
    ")\n",
    "\n",
    "seq.add(\n",
    "    ConvLSTM2D(\n",
    "        filters=30, \n",
    "        kernel_size=(3, 3),\n",
    "        padding='same', \n",
    "        return_sequences=True\n",
    "    )\n",
    ")\n",
    "\n",
    "seq.add(\n",
    "    Conv3D(\n",
    "        filters=15, \n",
    "        kernel_size=(3, 3, 3),\n",
    "        activation='relu',\n",
    "        padding='same'    \n",
    "    )\n",
    ")\n",
    "\n",
    "seq.add(\n",
    "    Conv3D(\n",
    "        filters=1, \n",
    "        kernel_size=(3, 3, 3),\n",
    "        activation='relu',\n",
    "        padding='same'\n",
    "    )\n",
    ")\n",
    "\n",
    "seq.add(Flatten())\n",
    "seq.add(Dense(Y_jan_train.shape[1], activation='relu'))\n",
    "\n",
    "# Compile model\n",
    "seq.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "\n",
    "# Print model summary\n",
    "seq.summary()\n",
    "\n",
    "# Train model\n",
    "print(f\"\\nTraining model...\")\n",
    "epochs = 100 \n",
    "batch_size = 4\n",
    "history = seq.fit(\n",
    "    X_exp2_train_scaled, Y_jan_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "print(f\"\\nEvaluating model...\")\n",
    "test_loss = seq.evaluate(X_exp2_test_scaled, Y_jan_test, verbose=0)\n",
    "print(f\"Test MAE: {test_loss:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = seq.predict(X_exp2_test_scaled, verbose=0)\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "mae = mean_absolute_error(Y_jan_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(Y_jan_test, y_pred))\n",
    "r2 = r2_score(Y_jan_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "if 'val_loss' in history.history:\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.legend()\n",
    "plt.title('Experiment 4: No-fire January set + HRRR\\nTraining Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MAE)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "results_dir = os.path.join(output_dir, \"experiment4\", \"results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "np.save(os.path.join(results_dir, \"y_pred.npy\"), y_pred)\n",
    "seq.save(os.path.join(results_dir, \"model.h5\"))\n",
    "\n",
    "exp2_results = {\n",
    "    'model': seq,\n",
    "    'history': history,\n",
    "    'loss': test_loss,\n",
    "    'mae': mae,\n",
    "    'rmse': rmse,\n",
    "    'r2': r2,\n",
    "    'y_pred': y_pred,\n",
    "    'y_test': Y_jan_test\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a464ca9a-7130-4c6e-a342-5e8c5a9baa11",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa443727-40fc-46e0-9310-e39639290d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nDetailed analysis for Experiment 4:\")\n",
    "X_test = X_exp2_test_scaled\n",
    "y_test = Y_jan_test\n",
    "y_pred = exp2_results['y_pred']\n",
    "model = exp2_results['model']\n",
    "description = \"No-fire January set + HRRR, two weeks\"\n",
    "\n",
    "print(f\"Analyzing Experiment 4: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b3905-4697-441f-a66f-635d8c90221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.plotting import (\n",
    "    plot_prediction_comparison,\n",
    "    plot_scatter_comparison,\n",
    "    plot_error_by_sensor,\n",
    "    plot_time_series_comparison,\n",
    "    plot_input_frames,\n",
    "    print_metrics\n",
    ")\n",
    "\n",
    "# Sensor names (use AirNow sensor names if available)\n",
    "if hasattr(jan_airnow, 'sensor_names') and jan_airnow.sensor_names is not None:\n",
    "    sensor_names = jan_airnow.sensor_names\n",
    "else:\n",
    "    sensor_names = [\n",
    "        \"North Holywood\", \n",
    "        \"Los Angeles - N. Main Street\", \n",
    "        \"Compton\",\n",
    "        \"Crestline - Lake Gregory\",\n",
    "        \"Fontana - Arrow Highway\",\n",
    "        \"Glendora - Laurel\",\n",
    "        \"Lake Elsinore - W. Flint Street\",\n",
    "        \"Long Beach Signal Hill\",\n",
    "        \"Mira Loma - Van Buren\",\n",
    "        \"Reseda\",\n",
    "        \"Riverside - Rubidoux\",\n",
    "        \"Santa Clarita\",\n",
    "        \"Simi Valley - Cochran Street\",\n",
    "        \"Temecula (Lake Skinner)\"\n",
    "    ]\n",
    "\n",
    "print(\"\\n1. Plotting prediction comparison...\")\n",
    "plot_prediction_comparison(y_pred, y_test, sensor_names, sample_idx=8)\n",
    "\n",
    "print(\"\\n2. Plotting scatter comparison...\")\n",
    "plot_scatter_comparison(y_pred, y_test)\n",
    "\n",
    "print(\"\\n3. Plotting error by sensor...\")\n",
    "plot_error_by_sensor(y_pred, y_test, sensor_names)\n",
    "\n",
    "print(\"\\n4. Plotting time series comparison...\")\n",
    "plot_time_series_comparison(y_pred, y_test, sensor_names)\n",
    "    \n",
    "print(\"\\n5. Plotting time series with shifted predictions...\")\n",
    "plot_time_series_comparison(y_pred, y_test, sensor_names, shift_pred=1)\n",
    "\n",
    "print(\"\\n6. Printing metrics...\")\n",
    "print_metrics(y_pred, y_test, sensor_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_exp_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save experiment comparison\n",
    "with open(os.path.join(output_dir, 'experiment4_results.txt'), 'w') as f:\n",
    "    f.write(\"==== Experiment 4 Results ====\\n\")\n",
    "    f.write(f\"Experiment 4 (No-fire January + HRRR, two weeks): MAE = {exp2_results['mae']:.4f}, RMSE = {exp2_results['rmse']:.4f}, R² = {exp2_results['r2']:.4f}\\n\")\n",
    "    f.write(f\"\\nAnalysis completed on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(\"\\nExperiment 4 complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hysplitrevised",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
